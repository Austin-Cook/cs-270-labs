{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Backpropagation Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import arff\n",
        "from typing import Tuple\n",
        "from typing import Literal\n",
        "from tabulate import tabulate\n",
        "import csv\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IRIS_SETOSA = 0\n",
        "IRIS_VERSICOLOR = 1\n",
        "IRIS_VIRGINICA = 2\n",
        "FALSE, TRUE = 0, 1\n",
        "\n",
        "\n",
        "def load_iris() -> Tuple[ndarray, ndarray]:\n",
        "  \"\"\"\n",
        "  Purpose: Classification between 3 types of iris flowers\n",
        "\n",
        "  Features:\n",
        "    sepallength\t(REAL)\n",
        "    sepalwidth (REAL)\n",
        "    petallength (REAL)\n",
        "    petalwidth (REAL)\n",
        "\n",
        "  Classifications:\n",
        "    -> {Iris-setosa: 0, Iris-versicolor: 1, Iris-virginica: 2 }\n",
        "  \"\"\"\n",
        "  data_arff = arff.loadarff(\"iris.arff\")\n",
        "  data_df = pd.DataFrame(data_arff[0])\n",
        "  data_np = data_df.to_numpy()\n",
        "  X = data_np[:, :-1]\n",
        "  # y = data_np[:, -1]\n",
        "\n",
        "  # categorical -> indicator variables (create boolean for each possible option)\n",
        "  indicator_vars = pd.get_dummies(data_df['class']).to_numpy().astype(int)\n",
        "  y = indicator_vars\n",
        "\n",
        "  # convert strings/booleans to numbers\n",
        "  X = X.astype(float)\n",
        "  # y = np.where(y == b'Iris-setosa', IRIS_SETOSA, y)\n",
        "  # y = np.where(y == b'Iris-versicolor', IRIS_VERSICOLOR, y)\n",
        "  # y = np.where(y == b'Iris-virginica', IRIS_VIRGINICA, y)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "\n",
        "TRAIN, TEST = 0, 1\n",
        "MALE, FEMALE = 0, 1\n",
        "hid, hId, hEd, hAd, hYd, had, hOd, hod, hUd, hud, hed = range(11)\n",
        "ANDREW, BILL, DAVID, MARK, JO, KATE, PENNY, ROSE, MIKE, NICK, RICH, TIM, SARAH, SUE, WENDY = range(15)\n",
        "\n",
        "\n",
        "def load_vowel():\n",
        "  \"\"\"\n",
        "  Purpose: Classify Vowel Sounds (Given Accoustic Features)\n",
        "\n",
        "  Features:\n",
        "    Train or Test { Train: 0, Test: 1}\n",
        "    Speaker Number { Andrew: 0, Bill: 1, David: 2, Mark: 3, Jo: 4, Kate: 5,\n",
        "                     Penny: 6, Rose: 7, Mike: 8, Nick: 9, Rich: 10, Tim: 11,\n",
        "                     Sarah: 12, Sue: 13, Wendy: 14 }\n",
        "    Sex { Male: 0, Female: 1}\n",
        "    Feature 0 (real)\n",
        "    Feature 1 (real)\n",
        "    Feature 2 (real)\n",
        "    Feature 3 (real)\n",
        "    Feature 4 (real)\n",
        "    Feature 5 (real)\n",
        "    Feature 6 (real)\n",
        "    Feature 7 (real)\n",
        "    Feature 8 (real)\n",
        "    Feature 9 (real)\n",
        "\n",
        "  Classifications:\n",
        "    -> { hid: 0, hId: 1, hEd: 2, hAd: 3, hYd: 4, had: 5, hOd: 6, hod: 7, hUd: 8,\n",
        "         hud: 9, hed: 10 }\n",
        "  \"\"\"\n",
        "  data_arff = arff.loadarff(\"vowel.arff\")\n",
        "  data_df = pd.DataFrame(data_arff[0])\n",
        "  data_df = data_df.drop(columns=['Train or Test', 'Speaker Number'])\n",
        "  data_np = data_df.to_numpy()\n",
        "  X = data_np[:, :-1]\n",
        "  # y = data_np[:, -1]\n",
        "\n",
        "  # categorical -> indicator variables (create boolean for each possible option)\n",
        "  indicator_vars = pd.get_dummies(data_df['Class']).to_numpy().astype(int)\n",
        "  y = indicator_vars\n",
        "\n",
        "  # convert strings to numbers\n",
        "  # X = np.where(X == b'Andrew', ANDREW, X)\n",
        "  # X = np.where(X == b'Bill', BILL, X)\n",
        "  # X = np.where(X == b'David', DAVID, X)\n",
        "  # X = np.where(X == b'Mark', MARK, X)\n",
        "  # X = np.where(X == b'Jo', JO, X)\n",
        "  # X = np.where(X == b'Kate', KATE, X)\n",
        "  # X = np.where(X == b'Penny', PENNY, X)\n",
        "  # X = np.where(X == b'Rose', ROSE, X)\n",
        "  # X = np.where(X == b'Mike', MIKE, X)\n",
        "  # X = np.where(X == b'Nick', NICK, X)\n",
        "  # X = np.where(X == b'Rich', RICH, X)\n",
        "  # X = np.where(X == b'Tim', TIM, X)\n",
        "  # X = np.where(X == b'Sarah', SARAH, X)\n",
        "  # X = np.where(X == b'Sue', SUE, X)\n",
        "  # X = np.where(X == b'Wendy', WENDY, X)\n",
        "  X = np.where(X == b'Male', MALE, X)\n",
        "  X = np.where(X == b'Female', FEMALE, X)\n",
        "  # y = np.where(y == b'hid', hid, y)\n",
        "  # y = np.where(y == b'hId', hId, y)\n",
        "  # y = np.where(y == b'hEd', hEd, y)\n",
        "  # y = np.where(y == b'hAd', hAd, y)\n",
        "  # y = np.where(y == b'hYd', hYd, y)\n",
        "  # y = np.where(y == b'had', had, y)\n",
        "  # y = np.where(y == b'hOd', hOd, y)\n",
        "  # y = np.where(y == b'hod', hod, y)\n",
        "  # y = np.where(y == b'hUd', hUd, y)\n",
        "  # y = np.where(y == b'hud', hud, y)\n",
        "  # y = np.where(y == b'hed', hed, y)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def load_real_estate_valuation(flatten=True) -> Tuple[ndarray, ndarray]:\n",
        "  \"\"\"\n",
        "  From: UC Irvine ML Repository\n",
        "  Purpose: Use Regression to Estimate House Price\n",
        "\n",
        "  Features:\n",
        "    X1 transaction date (REAL)\n",
        "    X2 house age (REAL)\n",
        "    X3 distance to the nearest MRT station (REAL)\n",
        "    X4 number of convenience stores (INT)\n",
        "    X5 latitude (REAL)\n",
        "    X6 longitude (REAL)\n",
        "\n",
        "  Regression:\n",
        "    Y house price of unit area (REAL)\n",
        "  \"\"\"\n",
        "  # load csv file\n",
        "  with open('real_estate_valuation.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "  # convert to numby array, and extract X and y\n",
        "  data = np.array(data)\n",
        "  X = data[1:, 1:-1]\n",
        "  y = data[1:, -1:]\n",
        "  X = X.astype(float)\n",
        "  y = y.astype(float)\n",
        "  if flatten:\n",
        "    y = y.flatten()\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def plot_by_epochs(y_values: list, y_axis_title: str):\n",
        "    \"\"\"\n",
        "        Plots values on the y-axis with indexes (epochs) on the x-axis\n",
        "    Args:\n",
        "        y_values (array-like): A 1D array or list of y_values at every epoch\n",
        "    \"\"\"\n",
        "    plt.plot(np.arange(len(y_values)), y_values)\n",
        "    plt.title(y_axis_title + \" vs Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(y_axis_title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# X, y = load_iris()\n",
        "# print(\"X:\\n\", str(X))\n",
        "# print(\"y:\\n\", str(y))\n",
        "\n",
        "# X, y = load_vowel()\n",
        "# np.set_printoptions(edgeitems=50)\n",
        "# print(\"X:\\n\", str(X))\n",
        "# print(\"y:\\n\", str(y))\n",
        "\n",
        "# X, y = load_real_estate_valuation()\n",
        "# print(\"X:\\n\", str(X))\n",
        "# print(\"y:\\n\", str(y))"
      ],
      "metadata": {
        "id": "9xHJ5HckEIf0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 1 Avoiding Overfit: Early Stopping and Loss Regularization\n",
        "\n",
        "### 1.1 (10%) No overfit avoidance\n",
        "Train the sklearn [MLP classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) on the [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff).  Use 3 output nodes (1 per class). Expanding the one output variable into 3 is called one-hot encoding or dummy variable encoding. There are lots of ways to implement this including the Pandas get_dummies method. This experiment is set up to run a little longer to better see the effects of overfit.  Be patient as there are lots of hidden nodes and a high max iterations setting.\n",
        "\n",
        "Use default parameters except the following:\n",
        "- hidden_layer_sizes = [64] - One hidden layer with 64 hidden nodes\n",
        "- activation = 'logistic'\n",
        "- solver = 'sgd'\n",
        "- alpha = 0\n",
        "- batch_size = 1\n",
        "- learning_rate_init = 0.01\n",
        "- shuffle = True\n",
        "- momentum = 0\n",
        "- n_iter_no_change = 50\n",
        "- max_iterations = 1000\n",
        "\n",
        "Use a random 80/20 split of the data.  Run it a few times with different random training/test splits and give average values for\n",
        "- Number of iterations until convergence\n",
        "- Training set accuracy\n",
        "- Test set accuracy\n",
        "For one run observe the softmax probabilities on the test set using clf.predict_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rng8obxzfUC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74639a7-0b82-46f3-cb16-f1c5873c29c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial # 0\n",
            "   Num iterations:  308\n",
            "   Train set accuracy:  0.9916666666666667\n",
            "   Test set accuracy:  0.9333333333333333\n",
            "Trial # 1\n",
            "   Num iterations:  290\n",
            "   Train set accuracy:  0.975\n",
            "   Test set accuracy:  0.8\n",
            "Trial # 2\n",
            "   Num iterations:  220\n",
            "   Train set accuracy:  0.9833333333333333\n",
            "   Test set accuracy:  0.9666666666666667\n",
            "Trial # 3\n",
            "   Num iterations:  427\n",
            "   Train set accuracy:  0.975\n",
            "   Test set accuracy:  1.0\n",
            "Trial # 4\n",
            "   Num iterations:  288\n",
            "   Train set accuracy:  0.975\n",
            "   Test set accuracy:  0.9666666666666667\n",
            "Softmax probabilities using test data:\n",
            "[6.2 2.9 4.3 1.3]  ->  [1.62643875e-03 9.97558136e-01 2.88435453e-04]  ( [0 1 0]  actual)\n",
            "[7.2 3.2 6.  1.8]  ->  [1.50119008e-05 2.06417207e-01 8.93290727e-01]  ( [0 0 1]  actual)\n",
            "[5.6 3.  4.1 1.3]  ->  [1.28547490e-03 9.93294622e-01 6.00200405e-04]  ( [0 1 0]  actual)\n",
            "[5.1 2.5 3.  1.1]  ->  [2.21051641e-02 9.97756908e-01 4.14091119e-06]  ( [0 1 0]  actual)\n",
            "[4.8 3.  1.4 0.3]  ->  [9.98622225e-01 1.26899545e-02 1.10828733e-11]  ( [1 0 0]  actual)\n",
            "[5.5 4.2 1.4 0.2]  ->  [9.99756616e-01 1.65859996e-03 1.33348085e-12]  ( [1 0 0]  actual)\n",
            "[5.5 3.5 1.3 0.2]  ->  [9.99634113e-01 3.49321013e-03 2.05572108e-12]  ( [1 0 0]  actual)\n",
            "[4.5 2.3 1.3 0.3]  ->  [9.95432189e-01 7.49032243e-02 4.01840677e-11]  ( [1 0 0]  actual)\n",
            "[4.7 3.2 1.3 0.2]  ->  [9.99277890e-01 5.85855865e-03 5.13121457e-12]  ( [1 0 0]  actual)\n",
            "[5.8 4.  1.2 0.2]  ->  [9.99803539e-01 1.69354678e-03 9.44691644e-13]  ( [1 0 0]  actual)\n",
            "[5.9 3.  5.1 1.8]  ->  [1.36019046e-05 1.02777171e-01 9.29739557e-01]  ( [0 0 1]  actual)\n",
            "[5.  3.4 1.6 0.4]  ->  [9.98640963e-01 9.17214213e-03 1.20249753e-11]  ( [1 0 0]  actual)\n",
            "[5.5 2.4 3.7 1. ]  ->  [3.90637494e-03 9.98768217e-01 5.44861337e-05]  ( [0 1 0]  actual)\n",
            "[5.5 2.4 3.8 1.1]  ->  [1.97531413e-03 9.97333347e-01 2.15954170e-04]  ( [0 1 0]  actual)\n",
            "[5.8 2.7 4.1 1. ]  ->  [3.02242707e-03 9.98848720e-01 8.01857298e-05]  ( [0 1 0]  actual)\n",
            "[7.3 2.9 6.3 1.8]  ->  [5.62746870e-06 4.45115006e-02 9.86940300e-01]  ( [0 0 1]  actual)\n",
            "[7.7 3.  6.1 2.3]  ->  [4.52596701e-06 2.65140756e-02 9.92474503e-01]  ( [0 0 1]  actual)\n",
            "[6.7 3.3 5.7 2.1]  ->  [6.58325914e-06 3.67930121e-02 9.84504521e-01]  ( [0 0 1]  actual)\n",
            "[7.7 2.8 6.7 2. ]  ->  [2.40537723e-06 1.08353399e-02 9.97959509e-01]  ( [0 0 1]  actual)\n",
            "[7.4 2.8 6.1 1.9]  ->  [6.84010179e-06 6.20172648e-02 9.79997545e-01]  ( [0 0 1]  actual)\n",
            "[5.8 2.7 3.9 1.2]  ->  [2.92878085e-03 9.98207673e-01 9.97788792e-05]  ( [0 1 0]  actual)\n",
            "[6.3 3.4 5.6 2.4]  ->  [2.65235977e-06 6.38492707e-03 9.98028504e-01]  ( [0 0 1]  actual)\n",
            "[4.8 3.  1.4 0.1]  ->  [9.99018575e-01 1.17479983e-02 6.57789235e-12]  ( [1 0 0]  actual)\n",
            "[5.7 3.  4.2 1.2]  ->  [1.58044547e-03 9.96102743e-01 3.52708921e-04]  ( [0 1 0]  actual)\n",
            "[6.3 3.3 4.7 1.6]  ->  [3.69655273e-04 9.75798934e-01 7.24021276e-03]  ( [0 1 0]  actual)\n",
            "[5.  3.5 1.6 0.6]  ->  [9.98340115e-01 7.62183221e-03 1.78054089e-11]  ( [1 0 0]  actual)\n",
            "[4.8 3.4 1.9 0.2]  ->  [9.97357529e-01 2.60406785e-02 2.34899579e-11]  ( [1 0 0]  actual)\n",
            "[7.6 3.  6.6 2.1]  ->  [2.48157806e-06 1.02945813e-02 9.97908939e-01]  ( [0 0 1]  actual)\n",
            "[5.1 3.5 1.4 0.2]  ->  [9.99479987e-01 4.21828515e-03 3.36965554e-12]  ( [1 0 0]  actual)\n",
            "[5.9 3.2 4.8 1.8]  ->  [4.56530073e-05 4.52934099e-01 4.73920114e-01]  ( [0 1 0]  actual)\n",
            "\n",
            "Final Results:\n",
            "   Avg num iterations:  306.6\n",
            "   Avg train set accuracy:  0.9800000000000001\n",
            "   Avg test set accuracy:  0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "# Iris with no regularization\n",
        "NUM_TRIALS = 5\n",
        "avg_n_iter = 0.0 # num iterations for convergence\n",
        "avg_train_acc = 0.0 # average train set accuracy\n",
        "avg_test_acc = 0.0 # average test set accuracy\n",
        "\n",
        "X, y = load_iris()\n",
        "\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf = MLPClassifier(hidden_layer_sizes = [64],\n",
        "                      activation = 'logistic',\n",
        "                      solver = 'sgd',\n",
        "                      alpha = 0, # setting to 0 turns off loss regularization\n",
        "                      batch_size = 1,\n",
        "                      learning_rate_init = 0.01,\n",
        "                      shuffle = True,\n",
        "                      momentum = 0,\n",
        "                      n_iter_no_change = 50,\n",
        "                      max_iter = 1000)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # results\n",
        "  n_iter = clf.n_iter_\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  print(\"Trial #\", str(i))\n",
        "  print(\"   Num iterations: \", str(clf.n_iter_))\n",
        "  print(\"   Train set accuracy: \", str(train_acc))\n",
        "  print(\"   Test set accuracy: \", str(test_acc))\n",
        "\n",
        "  avg_n_iter += n_iter\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  # print softmax probabilities of last trial (% confidence in each prediction)\n",
        "  if i == NUM_TRIALS - 1:\n",
        "    print(\"Softmax probabilities using test data:\")\n",
        "    probs = clf.predict_proba(X_test)\n",
        "    for i in range(len(X_test)):\n",
        "      print(X_test[i], str(\" -> \"), probs[i], \" (\", y_test[i], \" actual)\")\n",
        "\n",
        "avg_n_iter /= NUM_TRIALS\n",
        "avg_train_acc /= NUM_TRIALS\n",
        "avg_test_acc /= NUM_TRIALS\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"   Avg num iterations: \", str(avg_n_iter))\n",
        "print(\"   Avg train set accuracy: \", str(avg_train_acc))\n",
        "print(\"   Avg test set accuracy: \", str(avg_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnfhuRoJfUC_"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "I used pd.get_dummies to do one hot encoding. Since the Iris dataset has three potential classifications, this created three columns, one cooresponding to each classification. Then I converted the array to numpy, casted the boolean values to integer values, and used them as my y-vector (targets). The terminology used in the spec was that these are indicator variables.\n",
        "\n",
        "Each of the five trials was able to run to convergence. The number of iterations seems realistic, varying from 220 to 427 iterations each. It was interesting to note that on trial #3, the test set actually scored perfectly. I believe this has to do with the fact that it was a lucky division of training and test data. Some inputs are easier to classify than others, and the test inputs in that trial probabily happened to get many of the easier to classify inputs.\n",
        "\n",
        "It is good to note that in all trials except for #3, the train set accuracy is higher than the test set accuracy. This is exactly what I would expect, because the model has already seen the training set and updated weights based on it, so it should be able to fit it well.\n",
        "\n",
        "Looking at the softmax probabilities (predictions) was very insightful. I also printed out the y_test-vector alongside each prediction to see how accurate the probabilities are. Looking through a few, I saw that the probability that was much higher than the other two always cooresponded to the correct classification.\n",
        "\n",
        "One thing that confused me was that the values for each softmax probabilities didn't always add up to 1, though they were typically fairly close. Though it could be because of an error on my part, it could have to do with how the softmax calculates the probabilities or rounding error, but I will look more into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rMJzWkefUDA"
      },
      "source": [
        "### 1.2 (10%) Early Stopping (Validation Set)\n",
        "\n",
        "- Do the same as above but his time with early stopping\n",
        "- Use a validation set taken from the training set for your stopping criteria. Using 10-15% of the training set for a validation set is common. You do this simply by setting the MLPClassifier early_stopping, validation_fraction, and n_iter_no_change parameters.\n",
        "- Run it a few times with different training/test splits and give average values for\n",
        "    - Number of iterations until convergence\n",
        "    - Training set accuracy\n",
        "    - Test set accuracy\n",
        "    - Best validation score (MLPClassifer attribute best_validation_score_)\n",
        "- For one run create a graph with validation set accuracy (*y*-axis) vs epochs (*x*-axis). Hint: MLPClassifer attribute validation_scores_\n",
        "\n",
        "Note: Due to the simplicity of and lack of noise in the iris data set you will not see the accuracy improvements that early stopping or loss regularization can give for more complex noisy datasets.  In particular, early stopping will have lower than expected results because with a very small VS taken from a very small training set there is less data to train on and more variance with the VS score.  Thus, you will probably get lower accuracies for VS than normal training for this less typical case.  But at least you will get practice on using early stopping and loss regularization for future data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM1Yf05IfUDA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35147850-be33-4a3d-bba9-741d35133ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial # 0\n",
            "   Num iterations:  118\n",
            "   Train set accuracy:  0.9666666666666667\n",
            "   Test set accuracy:  0.9333333333333333\n",
            "   Best validation score:  1.0\n",
            "Trial # 1\n",
            "   Num iterations:  142\n",
            "   Train set accuracy:  0.9416666666666667\n",
            "   Test set accuracy:  0.9\n",
            "   Best validation score:  1.0\n",
            "Trial # 2\n",
            "   Num iterations:  60\n",
            "   Train set accuracy:  0.675\n",
            "   Test set accuracy:  0.6333333333333333\n",
            "   Best validation score:  0.8666666666666667\n",
            "Trial # 3\n",
            "   Num iterations:  105\n",
            "   Train set accuracy:  0.9166666666666666\n",
            "   Test set accuracy:  0.8\n",
            "   Best validation score:  1.0\n",
            "Trial # 4\n",
            "   Num iterations:  159\n",
            "   Train set accuracy:  0.9833333333333333\n",
            "   Test set accuracy:  1.0\n",
            "   Best validation score:  1.0\n",
            "Softmax probabilities using test data:\n",
            "[7.1 3.  5.9 2.1]  ->  [8.70487260e-05 1.90530915e-01 9.46450367e-01]  ( [0 0 1]  actual)\n",
            "[5.9 3.  4.2 1.5]  ->  [0.00407327 0.75993589 0.03841338]  ( [0 1 0]  actual)\n",
            "[4.8 3.  1.4 0.3]  ->  [9.94863997e-01 7.39957241e-02 5.23101469e-08]  ( [1 0 0]  actual)\n",
            "[7.7 2.6 6.9 2.3]  ->  [1.63970260e-05 5.49542411e-02 9.96300754e-01]  ( [0 0 1]  actual)\n",
            "[4.4 3.  1.3 0.2]  ->  [9.96083185e-01 5.54747109e-02 4.06200125e-08]  ( [1 0 0]  actual)\n",
            "[5.7 2.9 4.2 1.3]  ->  [0.00422602 0.76559577 0.03648785]  ( [0 1 0]  actual)\n",
            "[6.4 3.2 4.5 1.5]  ->  [0.00444541 0.80989989 0.03022222]  ( [0 1 0]  actual)\n",
            "[6.3 2.3 4.4 1.3]  ->  [0.00195226 0.78246706 0.0950003 ]  ( [0 1 0]  actual)\n",
            "[6.7 3.3 5.7 2.1]  ->  [1.22467089e-04 2.11635438e-01 9.15042873e-01]  ( [0 0 1]  actual)\n",
            "[5.4 3.9 1.3 0.4]  ->  [9.99070674e-01 2.11849850e-02 7.38466759e-09]  ( [1 0 0]  actual)\n",
            "[5.6 2.7 4.2 1.3]  ->  [0.00270394 0.72151062 0.07109666]  ( [0 1 0]  actual)\n",
            "[5.5 4.2 1.4 0.2]  ->  [9.99388962e-01 1.71134464e-02 4.39943486e-09]  ( [1 0 0]  actual)\n",
            "[7.  3.2 4.7 1.4]  ->  [0.00619553 0.88066623 0.01566933]  ( [0 1 0]  actual)\n",
            "[4.9 3.1 1.5 0.1]  ->  [9.96080345e-01 7.12248719e-02 3.60866653e-08]  ( [1 0 0]  actual)\n",
            "[6.1 2.8 4.7 1.2]  ->  [0.00190781 0.7411277  0.1042676 ]  ( [0 1 0]  actual)\n",
            "[5.5 2.3 4.  1.3]  ->  [0.00213937 0.70755709 0.09891674]  ( [0 1 0]  actual)\n",
            "[6.9 3.1 4.9 1.5]  ->  [0.00245641 0.80463183 0.06544617]  ( [0 1 0]  actual)\n",
            "[5.  3.  1.6 0.2]  ->  [9.92840544e-01 1.07149136e-01 7.10907235e-08]  ( [1 0 0]  actual)\n",
            "[6.3 2.5 4.9 1.5]  ->  [5.05527014e-04 5.29593036e-01 4.96376399e-01]  ( [0 1 0]  actual)\n",
            "[6.3 3.3 4.7 1.6]  ->  [0.00225943 0.71884938 0.08638042]  ( [0 1 0]  actual)\n",
            "[5.2 4.1 1.5 0.1]  ->  [9.99188124e-01 1.97914201e-02 6.24407389e-09]  ( [1 0 0]  actual)\n",
            "[6.5 3.  5.8 2.2]  ->  [5.08267620e-05 1.09631115e-01 9.79159147e-01]  ( [0 0 1]  actual)\n",
            "[5.8 2.7 3.9 1.2]  ->  [0.00945679 0.84725148 0.01026399]  ( [0 1 0]  actual)\n",
            "[4.9 2.5 4.5 1.7]  ->  [1.78696531e-04 2.02730913e-01 8.78119754e-01]  ( [0 0 1]  actual)\n",
            "[5.6 3.  4.1 1.3]  ->  [0.00587343 0.7728609  0.02318904]  ( [0 1 0]  actual)\n",
            "[6.9 3.1 5.4 2.1]  ->  [2.24658807e-04 3.39173365e-01 7.90214566e-01]  ( [0 0 1]  actual)\n",
            "[7.9 3.8 6.4 2. ]  ->  [2.39448607e-04 3.98299968e-01 7.49835635e-01]  ( [0 0 1]  actual)\n",
            "[5.6 2.8 4.9 2. ]  ->  [1.32425867e-04 1.84641194e-01 9.15754180e-01]  ( [0 0 1]  actual)\n",
            "[5.8 2.8 5.1 2.4]  ->  [5.65848936e-05 1.01061966e-01 9.77524615e-01]  ( [0 0 1]  actual)\n",
            "[5.1 3.8 1.5 0.3]  ->  [9.98383818e-01 2.86592507e-02 1.43296616e-08]  ( [1 0 0]  actual)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results:\n",
            "   Avg num iterations:  116.8\n",
            "   Avg train set accuracy:  0.8966666666666665\n",
            "   Avg test set accuracy:  0.8533333333333333\n",
            "   Avg best validation score:  0.9733333333333334\n"
          ]
        }
      ],
      "source": [
        "#Iris with early stopping and validation scores graph\n",
        "# Early stopping - specify the 3 parameters:\n",
        "# (1) Set early stopping = True\n",
        "# (2) Select % of training data to be validation set (10%-20% is normal\n",
        "# (3) Select max # iterations no change\n",
        "NUM_TRIALS = 5\n",
        "avg_n_iter = 0.0 # num iterations for convergence\n",
        "avg_train_acc = 0.0\n",
        "avg_test_acc = 0.0\n",
        "avg_best_validation_score = 0.0\n",
        "\n",
        "X, y = load_iris()\n",
        "\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf = MLPClassifier(hidden_layer_sizes = [64],\n",
        "                      activation = 'logistic',\n",
        "                      solver = 'sgd',\n",
        "                      alpha = 0,\n",
        "                      batch_size = 1,\n",
        "                      learning_rate_init = 0.01,\n",
        "                      shuffle = True,\n",
        "                      momentum = 0,\n",
        "                      n_iter_no_change = 50, # used for early stopping\n",
        "                      max_iter = 1000,\n",
        "                      early_stopping = True, # new\n",
        "                      validation_fraction = 0.12) # new\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # results\n",
        "  n_iter = clf.n_iter_\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "  best_validation_score = clf.best_validation_score_ # new\n",
        "\n",
        "  print(\"Trial #\", str(i))\n",
        "  print(\"   Num iterations: \", str(clf.n_iter_))\n",
        "  print(\"   Train set accuracy: \", str(train_acc))\n",
        "  print(\"   Test set accuracy: \", str(test_acc))\n",
        "  print(\"   Best validation score: \", str(best_validation_score)) # new\n",
        "\n",
        "  avg_n_iter += n_iter\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "  avg_best_validation_score += best_validation_score\n",
        "\n",
        "  if i == NUM_TRIALS - 1:\n",
        "    # print softmax probabilities\n",
        "    print(\"Softmax probabilities using test data:\")\n",
        "    probs = clf.predict_proba(X_test)\n",
        "    for i in range(len(X_test)):\n",
        "      print(X_test[i], str(\" -> \"), probs[i], \" (\", y_test[i], \" actual)\")\n",
        "\n",
        "    # graph validation set accuracy vs epochs\n",
        "    plot_by_epochs(clf.validation_scores_, \"Validation Scores\")\n",
        "\n",
        "\n",
        "avg_n_iter /= NUM_TRIALS\n",
        "avg_train_acc /= NUM_TRIALS\n",
        "avg_test_acc /= NUM_TRIALS\n",
        "avg_best_validation_score /= NUM_TRIALS\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"   Avg num iterations: \", str(avg_n_iter))\n",
        "print(\"   Avg train set accuracy: \", str(avg_train_acc))\n",
        "print(\"   Avg test set accuracy: \", str(avg_test_acc))\n",
        "print(\"   Avg best validation score: \", str(avg_best_validation_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNMATZqjtlxs"
      },
      "source": [
        "*Discussion of average values above and the validation score graph*  \n",
        "\n",
        "Using early stopping yielded results that the note said would appear. For example, the average training and test set accuracies were actually lower than they were when not using early stopping. Without early stopping they were 98% and 93%, respectively, while with early stopping they were 89% and 85%, respectively. This is due to the fact that the Iris dataset is pretty small, so much of the little amount of data it taken up for the testing and validation sets.\n",
        "\n",
        "It is also important to note that the average number of iterations went down from 306 without early stopping to 116 with early stopping. This means that compute time was almost a third, and early stopping did take effect.\n",
        "\n",
        "It is interesting to note that the best validation score was 1.0 in 4 of the 5 trials. I think this is probably because, again, the data sets were quite small so it lucked out that way. Also, the validation set was taken from only a fraction of the training set, so that chance of getting lucky and easy to classify inputs in it are higher.\n",
        "\n",
        "Looking at the graph, it is important to not that on the last trial (the one used to make the graph), there were 159 iterations, so the graph shows the validation score at the end of each iteration. In the first 15 iteratoins or so, the score shoots up quickly, and I wasn't surprised at that because it is the principle of diminishing returns. It's interesting that between epochs 15 and 60, there really wasn't much improvement, but at epoch 60, it must have found its way out of a minima and instantly improved after which the improvements were small. N_iter_no_change was set to 50 so I the last period from about 110 to 160 was the inteval of 50 epochs where no significant improvement was seen so it stopped there at round 159."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqfwQtofUDA"
      },
      "source": [
        "### 1.3 (10%) Loss Regularization\n",
        "\n",
        "- Do the same as in 1.1 but his time with loss regularization (Do not do early stopping)\n",
        "- Run it with different L2 regularization parameter values (alpha).  The default for alpha is .0001.  Try other values such as .1, .01, .001, .00001, etc. Make a table with each row including:\n",
        "    - The regularization parameter value\n",
        "    - Number of iterations until convergence\n",
        "    - Training set accuracy\n",
        "    - Test set accuracy\n",
        "    - Best loss value (MLPClassifer attribute best_loss_)\n",
        "- Which regularization value gave you the best results?\n",
        "- For your best regularization value do one run and create a graph with loss (*y*-axis) vs epochs (*x*-axis) for the training set (Hint: MLPClassifer attribute loss_curve_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUcRyXHtfUDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d134df-a85b-45ba-e850-a94c6dd7d423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Trial    Alpha    Num Iters for Conv    Train Acc    Test Acc    Best Loss Score\n",
            "-------  -------  --------------------  -----------  ----------  -----------------\n",
            "      0   0.0001                   266     0.991667    0.933333           0.13784\n",
            "      1   0.001                    234     0.983333    0.966667           0.24723\n",
            "      2   0.01                     346     0.966667    0.966667           0.800134\n",
            "      3   0.1                      137     0.616667    0.733333           1.64376\n"
          ]
        }
      ],
      "source": [
        "#Iris with Loss Regularization\n",
        "\n",
        "# Regularization Value (% weight decay at each iteration)\n",
        "# higher alpha -> more generalization, higher chance of underfit\n",
        "# lower alpha -> less generalization, higher change of overfit\n",
        "alpha_values = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "X, y = load_iris()\n",
        "\n",
        "table = []\n",
        "for i, alpha in enumerate(alpha_values):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  clf = MLPClassifier(hidden_layer_sizes = [64],\n",
        "                      activation = 'logistic',\n",
        "                      solver = 'sgd',\n",
        "                      alpha = alpha, # loss regularization alpha value\n",
        "                      batch_size = 1,\n",
        "                      learning_rate_init = 0.01,\n",
        "                      shuffle = True,\n",
        "                      momentum = 0,\n",
        "                      n_iter_no_change = 50,\n",
        "                      max_iter = 1000)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # results\n",
        "  n_iter = clf.n_iter_\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "  best_loss = clf.best_loss_ # lowest loss value achieved (lower is better)\n",
        "\n",
        "  table.append([i, alpha, n_iter, train_acc, test_acc, best_loss])\n",
        "\n",
        "headers = [\"Trial\", \"Alpha\", \"Num Iters for Conv\", \"Train Acc\", \"Test Acc\",\n",
        "           \"Best Loss Score\"]\n",
        "print(tabulate(table, headers=headers))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_alpha = 0.001\n",
        "\n",
        "X, y = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes = [64],\n",
        "                    activation = 'logistic',\n",
        "                    solver = 'sgd',\n",
        "                    alpha = best_alpha, # loss regularization alpha value\n",
        "                    batch_size = 1,\n",
        "                    learning_rate_init = 0.01,\n",
        "                    shuffle = True,\n",
        "                    momentum = 0,\n",
        "                    n_iter_no_change = 50,\n",
        "                    max_iter = 1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# graph regularization loss vs epochs\n",
        "plot_by_epochs(clf.loss_curve_, \"Regularization Loss\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "iPnZ6KS0Gz4z",
        "outputId": "a0845c6a-ecb1-406f-a314-368c16e1b59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_p-tYrpfUDB"
      },
      "source": [
        "*Discussion and comparison between no regularization, early stopping and loss regularization*\n",
        "\n",
        "## Results from Loss Regularization\n",
        "\n",
        "I ran the MLPClassifier using alpha values (regularization values) from 0.0001 up to 0.1 by multiples of 10. Smaller alpha values mean that loss regularization will cause less generalization, while larger values lead to more generalization. With less generalization, there is a higher chance of overfit, while with more, there is a higher chance of underfit.\n",
        "\n",
        "I noticed that with a small alpha value (0.0001), the training set performed very well (99% accuracy), and the test set scored 93% accuracy. This is becasue with such a small alpha value, there is almost no generalization, so the model is very well fit to the test data, and isn't generalized well to fit the training data. Moving down the trials, as we increased the alpha value, the training accuracy slowly got worse, while the test accuracy got better. This is for the same reason explained above.\n",
        "\n",
        "In fact, look at trail 3 (alpha of 0.1). With such a high alpha, the generalization is very high, and the test set accuracy is actually higher than the training set accuracy. While this wouldn't necessarily happen every time (i.e. if we were to run with alpha of 0.1 1000 times and average the scores), and likely has to do with the fact that the test set isn't very big, it demonstrates that a higher alpha is leading to more generalization.\n",
        "\n",
        "The best loss score is the lowest loss value achieved. The trend here is that as alpha increases, the best loss value also increases. This means that with more generalization, it is more difficult to get low loss values in training. I don't think that this is necessarily an issue, because the point of generalization is to not simply fit well for the training data, but also for the test and real world data. Again, I think that either extreme (highest/lowest best loss score) is typically not actually the best, and it is somewhere in between.\n",
        "\n",
        "From these four trials, it appears that the best alpha was 0.001. While 0.0001 had a higher training set accuracy, that really isn't very relevant becuase it simply overfit the data, and training accuracy is more telling of how the model will perform on real world data. With 0.001, the test accuracy was the same as that of 0.01, but the training accuracy was higher, so I gave the win to 0.001. Of course, if I had done many trials for each alpha value and averaged the scores, the data would be more accurate and I would have a better idea of which for sure is better.\n",
        "\n",
        "Looking at the graph of regularization loss for the best alpha, it is interesting to see how similar it is to an exponential decay function, though the loss settles around 0.3 rather than at 0 ina regular decay function. From epochs 0 to about 15, there is very strong improvement, from epochs 15 to about 70, there is consistent improvement, and from epochs 70 to 300, there is still improvement, but the improvement decreases as time goes on and after epoch 250 or so, there really isn't much or any noticable improvement. N_iter_no_change is set to 50, so that checks out.\n",
        "\n",
        "## Comparison of (1) No Regularization, (2) Early Stopping with Validation Set, and (3) Early Stopping with Loss Regularizaiton\n",
        "\n",
        "Loss regularization outperformed the other two methods by quite a bit. Its test accuracy (which is the more important feature) was 96.67% which is pretty good. No regularization achieved 93.33% test accuracy, while early stopping with validation only scored 85.33% test accuracy. On training set accuracy, loss regularization also scored slightly higher than no regularization, and significantly higher than early stopping with validation, though this number doesn't tell much on how well it will perform on real world data. One thing to not is that with loss regularization, I only did 1 trial with the best regularizaiton value, so increasing that number could increase accuracy. Additionally, the test set was quite small, so early stopping with the validation set particularly took a hit, because the validation set was pretty small. It the data sets were higher, and I had done more trials, these results could have varied. From the limited trials I've done here however, it appears that using loss regularization was the best way to generalize the model to the correct amount and achieve the highest test set accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOteTlV6S0bq"
      },
      "source": [
        "## 2 Hyperparameters\n",
        "In this section we use the [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff) to consider the hyperparameters of learning rate, number of hidden nodes, and momentum.\n",
        "\n",
        "### 2.1 (10%) Vowel Dataset Questions\n",
        "- Give the baseline accuracies for the Iris and Vowel datasets. Baseline accuracy is what you would get if the model just outputs the majority class of the data set (i.e. the output value which occurs most often). These two data sets are not great example for this as they have an equal amount of each class, which is not typical.\n",
        "- Discuss why the vowel data set will probably have lower accuracy than Iris.\n",
        "- Consider which of the vowel dataset's input features you should not use in training and discuss why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmq9GSbJS8k2"
      },
      "source": [
        "*Discuss these items here*  \n",
        "\n",
        "## Baseline Accuracies\n",
        "\n",
        "For the Iris dataset, there are three possible classifications (flowers) and since there are an equal number of each flower in the dataset, the baseline accuracy is simply `[0.33, 0.33, 0.33]` because it is split between the three.\n",
        "\n",
        "For the Vowel dataset, there are 11 possible classifications (sounds) and since there are an equal number of each classification in the dataset, the baseline accuracy is simply `[0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09]` because it is split between the three.\n",
        "\n",
        "## Why the Vowel Data Set Will Have Lower Accuracy\n",
        "\n",
        "The vowell dataset will have lower accuracy because there are more possible classifications. If the model is only as good as a random guess, its accuracy would be about 0.0909 becasue there is a lower chance of guessing the right classification. There are more ways it could guess wrong, and the space is larger.\n",
        "\n",
        "## Which Input Features to Not Include and Why\n",
        "\n",
        "Dropping: `Train or Test`, and `Speaker Number`\n",
        "\n",
        "There are a couple of featuers I decided to drop. First was the `Train or Test` column. This is completely irrelevant to being able to determine the classification. I will rather derive the train and test sets from a random split. I also decided to drop the `Speaker Number` column. While this may help improve accuracy with a test set derived from this data, it won't help in the future if we want other individuals that didn't help with testing to be able to use the model to make classifications with their voices.\n",
        "\n",
        "I am leaving `Gender` category because it will male and female voices are different, so this will help the model be able to understand distinctions between genders. I will also leave all the other featuers. Right now, I no nothing from them, becasue they are simply numerical values cooresponding to certain aspects of sounds, but I don't know anything about them from a static analysis. After training the model, I will likely know more about which features are more important and know enough to be able to drop some if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "### 2.2 (10%) Learning Rate\n",
        "Load the [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff). Drop any features which you explained above as being inappropriate for training.\n",
        "\n",
        "Hints: Consider the Pandas drop method for dropping columns. When you want to transform features in your data set there are lots of approaches. You could edit the arff file directly, or make the transforms in your code.  The Pandas replace method is nice for that. For example, if you wanted to change the vowel data set gender feature in a Pandas dataframe to 0/1 you could do the following:\n",
        "\n",
        "vowel_df['Sex'] = vowel_df['Sex'].str.decode('utf-8')   //Changes the byte code data into a normal string, b'Male' becomes \"Male\"\\\n",
        "vowel_df = vowel_df.replace('Male', 0)\\\n",
        "vowel_df = vowel_df.replace('Female', 1)\n",
        "\n",
        "- Use one layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
        "- Use a random 75/25 split of the data for the training/test set.\n",
        "- Do not use early stopping.\n",
        "- Try at least 5 different learning rates (LR) from very small (e.g. .001) to pretty big (e.g. 10). Each LR will require a different number of epochs to learn. LR effects both accuracy and time required for learning.\n",
        "- Create a table which includes a row for each LR.  Your table columns should be LR, # epochs to learn the model, final training set accuracy and final test set accuracy.  As learning rates get smaller, it usually takes more epochs to learn. If your model is stopping learning too soon (converging) by hitting max_iterations (in this case and in experiments below), then you need to increase your max_iterations parameter in order to give your model a fair chance.  To keep things faster, you don't need to increase max_iter past 1000 if you don't want to, but point out when more iterations may have given improvement.\n",
        "\n",
        "In real testing one averages the results of multiple trials per LR (and other parameters) with different intitial conditions (training/test split, initial weights, etc.). That gives more accurate results but is not required for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBGUn43ASiXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba82baa-1943-428a-9d31-ca637332a09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Trial    Learning Rate    Num Epochs for Conv    Train Acc    Test Acc\n",
            "-------  ---------------  ---------------------  -----------  ----------\n",
            "      0            0.001                   1000    0.513477    0.483871\n",
            "      1            0.01                    1000    0.981132    0.846774\n",
            "      2            0.1                      731    0.997305    0.854839\n",
            "      3            1                        212    0.540431    0.532258\n",
            "      4           10                        108    0.0916442   0.0887097\n"
          ]
        }
      ],
      "source": [
        "# Train with different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
        "\n",
        "X, y = load_vowel()\n",
        "\n",
        "table = []\n",
        "for i, learning_rate in enumerate(learning_rates):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "  clf = MLPClassifier(hidden_layer_sizes = [22], # 11 input features x2 = 22\n",
        "                      activation = 'logistic',\n",
        "                      solver = 'sgd',\n",
        "                      alpha = 0,\n",
        "                      batch_size = 1,\n",
        "                      learning_rate_init = learning_rate,\n",
        "                      shuffle = True,\n",
        "                      momentum = 0,\n",
        "                      n_iter_no_change = 50,\n",
        "                      max_iter = 1000)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # results\n",
        "  n_iter = clf.n_iter_\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  table.append([i, learning_rate, n_iter, train_acc, test_acc])\n",
        "\n",
        "headers = [\"Trial\", \"Learning Rate\", \"Num Epochs for Conv\", \"Train Acc\",\n",
        "           \"Test Acc\",]\n",
        "print(tabulate(table, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpjJBIwktlxt"
      },
      "source": [
        "*Discuss your table and the effect of different learning rates on both training time and accuracy*\n",
        "\n",
        "The learning rate is how fast the MLP updates weights. In general, updating weights in large increments may be dangerous because it can cause the algorithm to jump over a minima rather than settling into it. Having smaller updataes is often better because it allows the algorithm to arrive right in a local minima. The downside, however, is that it can take a long time to train because updates are so small. You can see this when looking at trials 0 and 1 with the smaller learning rates. Since I set my max_iter to 1000, both of these trials were unable to finish in time. You can tell that for learning rate `0.001`, training wasn't near done yet, becasue accuracy for train and test sets were both around 50%. Note that this isn't awefull becasuethere are 11 possible datasets, so the 50% is much better than the baseline 9.09% accuracy.\n",
        "\n",
        "For learning rate `0.01`, it appears that even though it still reached that maximum number of epochs, training was much done becasue the train accuracy was less than 2% off of 100%, and the test accuracy was almost as high as the best trial. The best results were seen with the learning rate `0.1`, where it performed almost perfectly on the train data and test accuracy was 85.5% which is better than any of the others.\n",
        "\n",
        "It is interesting to look at trials 3 and 4 with the larger learning rates, because we can see the effect of the learning rate being too big. In trial 3 with a learning rate of `1`, train and test accuracy were both only about 54% (remember that this is still much better than a random guess becasue there are 11 classification options). I am pretty sure the drop here has to do with the fact that it is jumping over the minima and not able to settle down in it well. Looking at trial 4 with a learning rate of `10`, the accuracy is no better than average. It is clear that the learning rate was way too big, and the weight updates were so large that they jumper right over the optimal weights.\n",
        "\n",
        "This data is a great example that it is better in general to have a smaller learning rate, but we have to be cogniscent about how much computing resources we have, because if the learning rate is too small, it will take too long to train. Its also important to note that in this trial, the learning rate was kept constant and never updated. A lot of time could have been saved by using a method such as invscaling or adaptive to adjust the learning rate as you learn. Also note that here, I split the training and testing data differently with each trial. I should have left them the same to give more consistent circumstances for each trial. I changed this in 2.3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-nUu5Txtlxt"
      },
      "source": [
        "### 2.3 (10%) Number of Hidden Nodes\n",
        "\n",
        "Using the best LR you discovered, experiment with different numbers of hidden nodes.\n",
        "\n",
        "- Start with 1 hidden node, then 2, and then double them for each test until you get no more improvement in accuracy.\n",
        "- Create a table just like above, except with # of hidden nodes rather than LR.\n",
        "\n",
        "In general, whenever you are testing a parameter such as # of hidden nodes, keep testing values until no more improvement is found. For example, if 20 hidden nodes did better than 10, you would not stop at 20, but would try 40, etc., until you no longer got improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqeA1iutlxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca8c060-6f8c-4dc8-e411-9d60de078af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Trial    Num Nodes    Num Epochs for Conv    Train Acc    Test Acc\n",
            "-------  -----------  ---------------------  -----------  ----------\n",
            "      0            1                    270     0           0\n",
            "      1            2                    225     0.138814    0.169355\n",
            "      2            4                    185     0.261456    0.258065\n",
            "      3            8                    367     0.668464    0.629032\n",
            "      4           16                    469     0.963612    0.766129\n",
            "      5           32                    754     1           0.907258\n",
            "      6           64                    464     1           0.927419\n",
            "      7          128                    476     1           0.907258\n",
            "      8          256                    508     1           0.927419\n",
            "      9          512                    525     1           0.919355\n"
          ]
        }
      ],
      "source": [
        "# Train with different numbers of hidden nodes\n",
        "best_learning_rate = 0.1 # determined in 2.2\n",
        "num_nodes_trials = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "\n",
        "X, y = load_vowel()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "table = []\n",
        "for i, num_nodes in enumerate(num_nodes_trials):\n",
        "  clf = MLPClassifier(hidden_layer_sizes = [num_nodes], # 11 input features x2 = 22\n",
        "                      activation = 'logistic',\n",
        "                      solver = 'sgd',\n",
        "                      alpha = 0,\n",
        "                      batch_size = 1,\n",
        "                      learning_rate_init = best_learning_rate,\n",
        "                      shuffle = True,\n",
        "                      momentum = 0,\n",
        "                      n_iter_no_change = 50,\n",
        "                      max_iter = 1000)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # results\n",
        "  n_iter = clf.n_iter_\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  table.append([i, num_nodes, n_iter, train_acc, test_acc])\n",
        "\n",
        "headers = [\"Trial\", \"Num Nodes\", \"Num Epochs for Conv\", \"Train Acc\",\n",
        "           \"Test Acc\",]\n",
        "print(tabulate(table, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLliSMtXtlxt"
      },
      "source": [
        "*Discuss your table and the effect of different numbers of hidden nodes on both training time and accuracy*\n",
        "\n",
        "I'm not seeing a huge coorelation between number of nodes and the number of epochs for convergence. However, it seems to more or less increase until there are 32 nodes where is maxes out, and then decreases some from there. However,I think the lack of a better patter could have to do with the fact that I only did one trial at each number of nodes, and only used one dataset without modifying the training and test divisions between iterations. Increasing those may show a more consistent pattern.\n",
        "\n",
        "It's interesting to note that the training set accuracy steadily increases until at 32 nodes where it and all trials after it achieved perfect accuracy for the training set. This doesn't tell much, because it could simply be due to overfit. Looking at the test accuracy, the best score was with `64` nodes. I doubled it three times after that, and while with 256 nodes, it tied for the same accuracy, there was no more improvement. At 92.47% accuracy, there is room for improvement, but considering there are 11 classifications, that is pretty good accuracy, and is better than what I was able to achieve with only fine tuning the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v72ryeHXtlxu"
      },
      "source": [
        "### 2.4 (10%) Momentum\n",
        "\n",
        "Try at least 5 different momentum terms between 0 and just less than 1 using the best number of hidden nodes and LR from your earlier experiments.\n",
        "\n",
        "- Create a table just like above, except with momentum values rather than LR or number of hidden nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiEBTL6Vtlxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93e0def-1a63-436d-d520-9ec53e204c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Trial    Momentum    Num Epochs for Conv    Train Acc    Test Acc\n",
            "-------  ----------  ---------------------  -----------  ----------\n",
            "      0        0                       512    1            0.931452\n",
            "      1        0.25                    417    1            0.939516\n",
            "      2        0.5                     322    1            0.951613\n",
            "      3        0.75                    180    0.715633     0.629032\n",
            "      4        0.99                    142    0.0876011    0.100806\n"
          ]
        }
      ],
      "source": [
        "# Train with different momentum values\n",
        "best_learning_rate = 0.1 # from 2.2\n",
        "best_num_hidden_nodes = 64 # from 2.3\n",
        "\n",
        "momentums = [0, 0.25, 0.5, 0.75, 0.99]\n",
        "\n",
        "X, y = load_vowel()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "table = []\n",
        "for i, momentum in enumerate(momentums):\n",
        "  clf = MLPClassifier(hidden_layer_sizes = [best_num_hidden_nodes],\n",
        "                      activation = 'logistic',\n",
        "                      solver = 'sgd',\n",
        "                      alpha = 0,\n",
        "                      batch_size = 1,\n",
        "                      learning_rate_init = best_learning_rate,\n",
        "                      shuffle = True,\n",
        "                      momentum = momentum,\n",
        "                      n_iter_no_change = 50,\n",
        "                      max_iter = 1000)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # results\n",
        "  n_iter = clf.n_iter_\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  table.append([i, momentum, n_iter, train_acc, test_acc])\n",
        "\n",
        "headers = [\"Trial\", \"Momentum\", \"Num Epochs for Conv\", \"Train Acc\",\n",
        "           \"Test Acc\",]\n",
        "print(tabulate(table, headers=headers))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqidhekCtlxu"
      },
      "source": [
        "*Discuss your table and the effect of momentum on both training time and accuracy*\n",
        "\n",
        "Momentum is a hyperparameter that can lead help training occur faster. When changes repeatedly occur in the same direction, momentum speeds up how fast the weights update. The possible values for momentum range from 0 to just under 1. At zero, there is simply no momentum effect.\n",
        "\n",
        "Looking atthe results, I noticed that increasing momentum always decreased the number of epochs for convergence. This is what I would expect, because momentum simply helps you move quicker if you are going in the right direction. From momentum being 0 to 0.99, the number of epochs was cut to less than a third.\n",
        "\n",
        "Looking at accuracies, there was a sweet spot. For training set accuracy, it was perfect until momoentum got too big. For 0.75 and especially 0.99, accuracy decreased significantly. I think this is because you end up jumping too far over the best minima.\n",
        "\n",
        "For the test set accuracy, interestingly, the sweet spot was with a momentum of 0.5. Going higher than that led to significany losses in accuracy, while going lower only lost a maximum of about 2%. This demonstrates that it is safer to air on a lower momentum value, though of course, these trials are limited and using larger data sets and more trials would give more insight here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hHxNgUCtlxv"
      },
      "source": [
        "### 2.5 (10%) Automatic Hyperparameter Discovery\n",
        "Using the vowel dataset, automatically adjust the LR, # of hidden nodes, and momentum using [grid and random search](https://scikit-learn.org/stable/modules/grid_search.html)\n",
        "- For grid search include the most promising hyperparameter values you used in your experiments above.  You may add others also.\n",
        "- Be patient as the grid search can take a while since it has to train all combinations of models. Don't use too many parameter options or it will too slow.\n",
        "- Report your best hyperparameters and accuracy.  Unfortunately, you will not always get as high a score as you might expect.  This is in part due to the simplicity of the dataset.  It also teaches that in gerneral you should not always blindly assume that a tool will get you the results you expect and that you may need to consider multiple approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ae_z5q3xfUDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0329c9a-8d20-4bb2-8d42-d9838f4685f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hidden_layer_sizes': [128], 'learning_rate_init': 0.1, 'momentum': 0.25}\n",
            "0.4404040404040404\n"
          ]
        }
      ],
      "source": [
        "#Grid search for hyperparameters.\n",
        "#Here is one variation of code you could use for your grid search. You can try your own variation if you prefer.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "X, y = load_vowel()\n",
        "\n",
        "clf = MLPClassifier(activation='logistic', solver='sgd',alpha=0,early_stopping=True, n_iter_no_change=10, batch_size=1)\n",
        "parameters = {'learning_rate_init':(.01, .1, 1),\n",
        "              'hidden_layer_sizes': ([32], [64], [128]),\n",
        "              'momentum':(0, .25, .5)}\n",
        "grid = GridSearchCV(clf, parameters)\n",
        "grid.fit(X,y) #This takes a while to run\n",
        "print(grid.best_params_)\n",
        "print(grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TWEwYWBCfUDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f152b9-be84-4843-ddc6-e2cb676722f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hidden_layer_sizes': [128], 'learning_rate_init': 0.36862805837804963, 'momentum': 0.33408090406386265}\n",
            "0.3494949494949495\n"
          ]
        }
      ],
      "source": [
        "#Randomized search for hyperparameters\n",
        "#Here is one variation of code you could use for your randomized search.\n",
        "\n",
        "X, y = load_vowel()\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "clf = MLPClassifier(activation='logistic', solver='sgd',alpha=0,early_stopping=True, n_iter_no_change=10, batch_size=1)\n",
        "distributions = dict(learning_rate_init=uniform(loc=0.01, scale=.99), #loc is the min val, and loc + scale is the max val\n",
        "                    hidden_layer_sizes = ([32], [64], [128]), #since there is no distribution it samples these values uniformly\n",
        "                    momentum=uniform(loc=0,scale =.99))\n",
        "search = RandomizedSearchCV(clf, distributions, n_iter=10)\n",
        "search.fit(X,y)\n",
        "print(search.best_params_)\n",
        "print(search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Discussion and comparison of grid and randomized parameter search*\n",
        "\n",
        "For both grid search and randomized search, results really weren't that great. Grid search ended up with 44% accuracy, and randomized search had 35% accuracy. It is important to remember that there are 11 different classification, so both of those are much better than average, but by trying out parameters on my own, I achieved about 95% accuracy. I noticed that on grid search, the best results came with using lr 0.1 which was the best I had found in my own testing in previoius problems. However, for randomized, it was 0.37 which I am sure that with more trials, a different value (perhaps closer to 0.1) would be returned for that. The momentums returned by each was 0.25, and 0.33, respectively which, looking at my chart from 2.4, I can see that those are likely good values.\n",
        "\n",
        "Though the results were not great, I believe that they could be improved. For grid search, that would involve adding more possible hyperparameter values to try. For randomized search, it would mean increasing the iterations to more than 10. I think my biggest takeaway from this problem is that it can take a lot of trial and error to find the best hyperparameters, but using what intuition you have about the meaning of each hyperparameter can help you get to a good configuration more quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIM0rEA9tlxu"
      },
      "source": [
        "## 3 Regression with MLPs\n",
        "\n",
        "### 3.1 (10%) - Learn a regression data set of your choice\n",
        "\n",
        "Train MLP on any real world data set that requires regression (i.e. has a real valued ouput) and discuss your effort and results.  While the [Irvine ML Repository](https://archive.ics.uci.edu) is a great resource, also onsider [Kaggle](https://www.kaggle.com) and [OpenML](https://openml.org) as other great place to find datasets.\n",
        "- Use [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) rather than MLPclassifier.  It has almost the exact same setup as MLPClassier except it uses the linear activation function for the output nodes and SSE as the loss function.  MLPClassier uses softmax activation for the output nodes and cross-entropy for the loss function.\n",
        "- Use any reasonable hyperparameters that you want.  \n",
        "- You will probably need to normalize input features.\n",
        "- It is not typically necessary to normalize the output.\n",
        "- Split into train and test and report the training and test set MAEs (Mean Absolute Error). For regression problems where we don't normalize the output, MAE is an intuitive measure as it shows exactly how much our output is off on average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFQv70W2VyqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee8bb46-4035-4930-abac-8c94da260368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set MAE:  3.8054130246178097\n",
            "Test Set MAE:  6.198608561093369\n"
          ]
        }
      ],
      "source": [
        "# Load and Learn a real world regression data set\n",
        "X, y = load_real_estate_valuation()\n",
        "X = MinMaxScaler().fit_transform(X) # normalize X\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = MLPRegressor(hidden_layer_sizes = [64],\n",
        "                    activation = 'logistic',\n",
        "                    solver = 'sgd',\n",
        "                    alpha = 0,\n",
        "                    batch_size = 1,\n",
        "                    learning_rate_init = 0.01,\n",
        "                    shuffle = True,\n",
        "                    momentum = 0,\n",
        "                    n_iter_no_change = 50,\n",
        "                    max_iter = 1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# mean absolute error (how much the output is off on average)\n",
        "print(\"Train Set MAE: \", mean_absolute_error(clf.predict(X_train), y_train))\n",
        "print(\"Test Set MAE: \", mean_absolute_error(clf.predict(X_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqPTreAfUDC"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "For regression, I chose to use a real estate pricing dataset from the UC Irvine ML Repository. The dataset has six different features in X, and and the regression value is the cost of the house per area unit in Taiwanese dollars.\n",
        "\n",
        "To normalize the input values (X), I used Scikit-learn's MinMaxScalar. It's default is to normalize values between 0 and 1. This was easy to figure out and use.\n",
        "\n",
        "For the parameters, I used all the same parameters used in 1.1 with MLPClassifier. I read through the documentation's description of each one to ensure that the value made sense for the Regressor, and everything checked out.\n",
        "\n",
        "To show model performane, I used mean absolute error. This is similar to mean squared error, but it doesn't square each error, so the result can be taken at face value. For example, the MAE on the training set was 3.80. The cost of houses per area unit (in Taiwanese dollars) varied between about 20 to 60, so an MAE of 3.80 means that the model was off 3.80 Taiwanese dollars per area unit on average. The test set MAE was 6.19 meaning that the prediction was off by an average of 6.19 Taiwanese dollars per area unit. The numbers appear to be realistic, as I would expect the predictions to be more accurate on the training data, because it has been seen before. Also, being off by 3 to 6 Taiwanese dollars per area unit seem about right when houses sold for 20 to 60 dollars per area unit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Rg1yJ6fUDD"
      },
      "source": [
        "### 3.2 (10%) - Other MLP Hyperparameters\n",
        "With the same data set, you may (not required) experiment with some of the hyperparameters you already did above (LR, hidden nodes, momentum, validation set parameters, regularization).  But for sure experiment with and discuss the results of the first two hyperparameters below (activation functions and multiple hidden layers).  We encourage you to experiment briefly with the others but they are not required.\n",
        "\n",
        "- different hidden layer activation functions (tanh, relu in addition to logistic) - Note that Sklean does not currently let you choose the output layer activation function.  It is automatically softmax for classification and linear for regression.\n",
        "- more than one hidden layer\n",
        "- solver - try adam and lbfgs in addition to sgd\n",
        "- batch size\n",
        "- learning rate adaptation - this is the schedule parameter which lets LR adapt during learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu1JE4vStlxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "642e7423-6f58-4bc5-9dab-6a61c1420e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avtivation Function: logistic\n",
            "   Num Iterations:  479\n",
            "   Train Set MAE:  4.395856580002049\n",
            "   Test Set MAE:  5.1787800398926045\n",
            "Avtivation Function: tanh\n",
            "   Num Iterations:  184\n",
            "   Train Set MAE:  4.774979283069192\n",
            "   Test Set MAE:  5.737246471689992\n",
            "Avtivation Function: relu\n",
            "   Num Iterations:  69\n",
            "   Train Set MAE:  6.4767721502855\n",
            "   Test Set MAE:  6.311839202420985\n",
            "Hidden Layers: 1x64 Nodes\n",
            "   Num Iterations:  407\n",
            "   Train Set MAE:  4.573273002818494\n",
            "   Test Set MAE:  5.179737623210011\n",
            "Hidden Layers: 2x64 Nodes\n",
            "   Num Iterations:  541\n",
            "   Train Set MAE:  3.817450429039524\n",
            "   Test Set MAE:  5.560997908827189\n",
            "Hidden Layers: 3x64 Nodes\n",
            "   Num Iterations:  458\n",
            "   Train Set MAE:  3.9846382561408094\n",
            "   Test Set MAE:  5.1578580525483355\n",
            "Solver: sgd\n",
            "   Num Iterations:  501\n",
            "   Train Set MAE:  4.286590577029622\n",
            "   Test Set MAE:  5.402109881431841\n",
            "Solver: adam\n",
            "   Num Iterations:  654\n",
            "   Train Set MAE:  3.577878046128675\n",
            "   Test Set MAE:  5.8923150071378645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: lbfgs\n",
            "   Num Iterations:  1000\n",
            "   Train Set MAE:  2.460354185998417\n",
            "   Test Set MAE:  7.809441180531832\n",
            "Batch Size: 1\n",
            "   Num Iterations:  555\n",
            "   Train Set MAE:  4.469932289546121\n",
            "   Test Set MAE:  4.964162873290322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 100\n",
            "   Num Iterations:  1000\n",
            "   Train Set MAE:  5.337750280761626\n",
            "   Test Set MAE:  5.240567375495366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 200\n",
            "   Num Iterations:  1000\n",
            "   Train Set MAE:  5.693043879294212\n",
            "   Test Set MAE:  5.4221376406515605\n",
            "Learning Rate Adaption: Constant\n",
            "   Num Iterations:  420\n",
            "   Train Set MAE:  4.691328359562175\n",
            "   Test Set MAE:  5.237882031296957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate Adaption: Invscaling\n",
            "   Num Iterations:  1000\n",
            "   Train Set MAE:  5.781119970498937\n",
            "   Test Set MAE:  5.495903984651197\n",
            "Learning Rate Adaption: Adaptive\n",
            "   Num Iterations:  1000\n",
            "   Train Set MAE:  4.199652718162061\n",
            "   Test Set MAE:  5.218147838451525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Run with different hyperparameters\n",
        "default_params = {\n",
        "    'hidden_layer_sizes': [64],\n",
        "    'activation': 'logistic',\n",
        "    'solver': 'sgd',\n",
        "    'alpha': 0,\n",
        "    'batch_size': 1,\n",
        "    'learning_rate_init': 0.01,\n",
        "    'shuffle': True,\n",
        "    'momentum': 0,\n",
        "    'n_iter_no_change': 50,\n",
        "    'max_iter': 1000\n",
        "}\n",
        "\n",
        "trials = [\n",
        "    [{'activation': 'logistic'}, \"Avtivation Function: logistic\"],\n",
        "    [{'activation': 'tanh'}, \"Avtivation Function: tanh\"],\n",
        "    [{'activation': 'relu'}, \"Avtivation Function: relu\"],\n",
        "    [{'hidden_layer_sizes': [64]}, \"Hidden Layers: 1x64 Nodes\"],\n",
        "    [{'hidden_layer_sizes': [64, 64]}, \"Hidden Layers: 2x64 Nodes\"],\n",
        "    [{'hidden_layer_sizes': [64, 64, 64]}, \"Hidden Layers: 3x64 Nodes\"],\n",
        "    [{'solver': 'sgd'}, \"Solver: sgd\"],\n",
        "    [{'solver': 'adam'}, \"Solver: adam\"],\n",
        "    [{'solver': 'lbfgs'}, \"Solver: lbfgs\"],\n",
        "    [{'batch_size': 1}, \"Batch Size: 1\"],\n",
        "    [{'batch_size': 100}, \"Batch Size: 100\"],\n",
        "    [{'batch_size': 200}, \"Batch Size: 200\"],\n",
        "    [{'learning_rate': 'constant'}, \"Learning Rate Adaption: Constant\"],\n",
        "    [{'learning_rate': 'invscaling'}, \"Learning Rate Adaption: Invscaling\"],\n",
        "    [{'learning_rate': 'adaptive'}, \"Learning Rate Adaption: Adaptive\"]\n",
        "]\n",
        "\n",
        "X, y = load_real_estate_valuation()\n",
        "X = MinMaxScaler().fit_transform(X) # normalize X\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "for trial in trials:\n",
        "  trial_params, trial_title = trial[0], trial[1]\n",
        "  params = {**default_params, **trial_params}\n",
        "\n",
        "  clf = MLPRegressor(**params)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  print(trial_title)\n",
        "  print(\"   Num Iterations: \", clf.n_iter_)\n",
        "  print(\"   Train Set MAE: \", mean_absolute_error(clf.predict(X_train), y_train))\n",
        "  print(\"   Test Set MAE: \", mean_absolute_error(clf.predict(X_test), y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HScVQasltlxv"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "I was able to experiment with several different parameters but for time, I will just discuss the two required ones (Activation function and multiple hidden layers).\n",
        "\n",
        "## Activation Function (logistic, tanh, relu)\n",
        "\n",
        "The activation function is used in the MLP algorithm to compute the output (z) from the input (net) for a node. Logistic is also known as the sigmoid function, and apart from values that are very close to the center, returns a number very close to 0 or 1. This can be really good for classifying values as one of two classes, but I was surprised to find that it performed better than both tanh and relu. Tanh is the hyperbolic tangent function. It also uses an s-curve like the sigmoid/logistic function, but its values go from -1 to 1. The biggest difference to the sigmoid function is the difference in the range and the fact that it centers around y = 0, as well as the fact that there is a steeper gradiant than the sigmid function. With tanh, the number of iterations was almost cut in third compared to that of the logistic function, but performance on both the training and test set dropped slightly (about 0.5 for both training and test). Relu uses the max of 0 and x, which means that from x <= 0, the y values is 0 (flat), then at x = y, there is an infinite straight line at a slope of 1. From what I read, relu is better for deeper networks, which may be why here, with only one hidden layer, the regressor actually performed worse than both other activation functions. One benefit, however, is that training only took 69 epochs vs 479 epochs with logistic and 180 epochs with tanh.\n",
        "\n",
        "## Multiple hidden layers ([64], [64, 64], [64, 64, 64])\n",
        "\n",
        "Hidden layers are the nodes in between the input and output layers. The way these are specified in MLPRegressor (and classifier) it to pass a list where each index represents a hidden layer, and the value at each index represents the number of nodes in that layer. I decided to experiment with three options, the first having one layer with 64 nodes, the second having two layers with 64 nodes in each, and the third having three layers with 64 nodes in each. Interestingly, increading the number of layers didn't increase the number of iterations till convergence. That could have to do with how the data ended up being split, and just the luck of it, but I'm sure it also has to do with the fact that when there are more nodes, every node is still visited in each iteration, so having more nodes doesn't necessarily hinder the number of iterations until convergence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 4. (Optional 20% extra credit) Code up your own MLP/Backprop learner\n",
        "Below is a scaffold you could use if you want. Requirements for this task:\n",
        "- Your model should support the methods shown in the example scaffold below.\n",
        "- Ability to create a network structure with at least one hidden layer and an arbitrary number of nodes. You may choose just one activation function for all hidden and output nodes if you want (e.g. logistic activation function where the loss is SSE rather than cross-entropy).\n",
        "- Random weight initialization with small random weights with 0 mean. Remember that every hidden and output node should have its own bias weight.\n",
        "- Use stochastic training updates: update weights after each training instance (i.e. not batch)\n",
        "- Option to include a momentum term\n",
        "- Your class can inherit from the relevant scikit-learn learners (e.g. data shuffling, etc.), but don't call any of the super methods to accomplish the core methods in the scaffold.\n",
        "- Run the Iris data set above with your Backprop version. Show and discuss your results and how they compare with the sklearn version.\n",
        "- Coding MLP is a good experience but is a little more challening that implementing other models so the extra credit points are higher than typical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rokMDC3Qtlxv"
      },
      "source": [
        "*Discuss your results and any differences*\n",
        "\n",
        "Implementing the MLP was a lot of work, but a lot of fun! I used the same activation function (logistic) as the homework problem, as well as the same error functions for both output and hidden layers. Because of this, I was able to compare results to the HW problem and ensure that it was functioning properly. If you scroll down to past the implementation, you can see where I ran my MLP on the homework problem and the results are the same.\n",
        "\n",
        "I also ran my MLP on the Iris dataset (also below) and got excellent results. It's training set score was almost 100%, and the test set score was 93%, though having messed around with parameters a bit, I did get ~97% at least once, which is good as I got with the Scikit-Learn MLP with loss regularization.\n",
        "\n",
        "I tried to break everything out into functions as much as possible, so hopefully, the code is easy to read and understand. An interesting decision that I had to make was how to implement my score function. In the process, I learned the distinction between `multi-class classification` where the classification is one of several classes and `multi-label classification`, where the classification can be true for any number of attributes. Since the Iris data set is multi-class classifcation, I implemented my score function that way. It simply picks the greatest value as the winner, and compares that to the expected value in y.\n",
        "\n",
        "As a note for understanding my implementation, I saw that state variable terminology could easily become complicated, so I decided to use `1` for the input layer, `2` for the hidden layer, and `3` for the output layer. This makes understanding the variables more inuitive when looking at a lot of them. Additionally, I use `W` for weights, `N` for net, `Z` for output, and `E` for error. My implementation only allows for one hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hn8n_iR8tlxv"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class MLP(BaseEstimator,ClassifierMixin):\n",
        "  \"\"\"\n",
        "  NOTE - The current score function is built for multi-class classification (an\n",
        "         output may be one of several classes), rather than multi-label\n",
        "         classification (an output may be any number of the labels). If you want\n",
        "         to perform multi-label classifcation, simply implement a score function\n",
        "         that assumes that any number of the outputs can be part of the\n",
        "         classification (rather than only one)\n",
        "\n",
        "  X:\n",
        "    [[1a, 1b, 1c],\n",
        "     [2a, 2b, 2c],\n",
        "     [3a, 3b, 3c],\n",
        "    ]\n",
        "  y: [[1], NOTE - y for each input set MUST be in an array\n",
        "      [2]\n",
        "      [3]]\n",
        "  Layer Representation:\n",
        "    output_layer:    [7]        width=len(y[0])\n",
        "    hidden_layer: [4, 5, 6_bias]  width=hidden_layer_widths + 1 (bias) (only 1 hidden layer)\n",
        "    input_layer:  [1, 2, 3_bias]  width=len(X[0]) + 1 (bias)\n",
        "  Weight Representation:\n",
        "    W_1_to_2: input -> hidden\n",
        "          4 5 6\n",
        "      1 | . . .\n",
        "      2 | . . .\n",
        "      3 | . . .\n",
        "    W_2_to_3: hidden -> output\n",
        "          7\n",
        "      4 | .\n",
        "      5 | .\n",
        "      6 | .\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,lr=.1, momentum=0, shuffle=True,hidden_layer_widths=8):\n",
        "    \"\"\" Initialize class with chosen hyperparameters.\n",
        "\n",
        "    Args:\n",
        "      lr (float): A learning rate / step size.\n",
        "      shuffle(boolean): Whether to shuffle the training data each epoch. DO NOT SHUFFLE for evaluation / debug datasets.\n",
        "      momentum(float): The momentum coefficent\n",
        "    Optional Args (Args we think will make your life easier):\n",
        "      hidden_layer_widths (list(int)): A list of integers which defines the width of each hidden layer if hidden layer is none do twice as many hidden nodes as input nodes. (and then one more for the bias node)\n",
        "      For example: input width 1, then hidden layer will be 3 nodes\n",
        "    Example:\n",
        "      mlp = MLP(lr=.2,momentum=.5,shuffle=False,hidden_layer_widths = [3,3]),  <--- this will create a model with two hidden layers, both 3 nodes wide\n",
        "    \"\"\"\n",
        "    self.hidden_layer_widths = hidden_layer_widths\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.shuffle = shuffle\n",
        "\n",
        "  def fit(self, X, y, initial_weights=None, max_iter=1000):\n",
        "    \"\"\" Fit the data; run the algorithm and adjust the weights to find a good solution\n",
        "\n",
        "    Args:\n",
        "      X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "      y (array-like): A 2D numpy array with the training targets\n",
        "    Optional Args (Args we think will make your life easier):\n",
        "      initial_weights (array-like): allows the user to provide initial weights\n",
        "    Returns:\n",
        "      self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "\n",
        "    \"\"\"\n",
        "    self.init_cache(initial_weights)\n",
        "\n",
        "    # compute epochs until 3 consequtive epochs have no weight change greater than TOL\n",
        "    self.tol = 0.001\n",
        "    n_epochs_below_tol, self.n_epochs = 0, 0\n",
        "    while n_epochs_below_tol < 50 and self.n_epochs < max_iter:\n",
        "      # run epoch\n",
        "      max_change = self.run_epoch(X, y)\n",
        "\n",
        "      # prepare for next epoch\n",
        "      self.n_epochs += 1\n",
        "      n_epochs_below_tol = n_epochs_below_tol + 1 if max_change < self.tol else 0\n",
        "      if self.shuffle:\n",
        "        X, y = shuffle(X, y)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\" Predict all classes for a dataset X\n",
        "    Args:\n",
        "      X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "    Returns:\n",
        "      array, shape (n_samples,)\n",
        "        Predicted target values per element in X.\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = X.shape[0], len(self.Z_3)\n",
        "    res = np.zeros((n_rows, n_cols))\n",
        "    for row in range(n_rows):\n",
        "      self.set_inputs(X[row, :])\n",
        "      self.propagate_forward()\n",
        "\n",
        "      for col in range(n_cols):\n",
        "        res[row, col] = self.Z_3[col]\n",
        "\n",
        "    return res\n",
        "\n",
        "  def score(self, X, y):\n",
        "    \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
        "\n",
        "    Args:\n",
        "      X (array-like): A 2D numpy array with data, excluding targets\n",
        "      y (array-like): A 2D numpy array with targets\n",
        "\n",
        "    Returns:\n",
        "      score : float\n",
        "        Mean accuracy of self.predict(X) wrt. y.\n",
        "    \"\"\"\n",
        "    z = self.predict(X)\n",
        "    rows, cols = z.shape[0], z.shape[1]\n",
        "\n",
        "    num_correct = 0\n",
        "    for row in range(rows):\n",
        "      max_i_t = np.argmax(y[row, :])\n",
        "      max_i_z = np.argmax(z[row, :])\n",
        "      if max_i_t == max_i_z:\n",
        "        num_correct += 1\n",
        "\n",
        "    return num_correct / rows\n",
        "\n",
        "  def init_cache(self, initial_weights: float | None):\n",
        "    n_nodes_1 = X.shape[1] + 1               # input nodes, +1 for bias\n",
        "    n_nodes_2 = self.hidden_layer_widths + 1 # hidden nodes, +1 for bias\n",
        "    n_nodes_3 = y.shape[1]                   # output nodes\n",
        "\n",
        "    # weight vectors\n",
        "    LOW, HIGH = -0.01, 0.01\n",
        "    SHAPE_W_1_TO_2 = (n_nodes_1, n_nodes_2 - 1)\n",
        "    SHAPE_W_2_TO_3 = (n_nodes_2, n_nodes_3)\n",
        "    if (initial_weights):\n",
        "      self.W_1_to_2 = np.full(fill_value=initial_weights, shape=SHAPE_W_1_TO_2)\n",
        "      self.W_2_to_3 = np.full(fill_value=initial_weights, shape=SHAPE_W_2_TO_3)\n",
        "    else:\n",
        "      self.W_1_to_2 = np.random.uniform(low=LOW, high=HIGH, size=SHAPE_W_1_TO_2)\n",
        "      self.W_2_to_3 = np.random.uniform(low=LOW, high=HIGH, size=SHAPE_W_2_TO_3)\n",
        "    self.delta_W_1_to_2 = np.zeros(SHAPE_W_1_TO_2)\n",
        "    self.delta_W_2_to_3 = np.zeros(SHAPE_W_2_TO_3)\n",
        "\n",
        "    # net vectors\n",
        "    self.N_2 = np.zeros(n_nodes_2 - 1) # no net for bias\n",
        "    self.N_3 = np.zeros(n_nodes_3)\n",
        "\n",
        "    # output vectors\n",
        "    self.Z_1 = np.zeros(n_nodes_1)\n",
        "    self.Z_2 = np.zeros(n_nodes_2)\n",
        "    self.Z_3 = np.zeros(n_nodes_3)\n",
        "    self.Z_1[-1], self.Z_2[-1] = 1, 1       # set bias Z\n",
        "\n",
        "    # error vectors\n",
        "    self.E_2 = np.zeros(n_nodes_2 - 1) # no error for bias\n",
        "    self.E_3 = np.zeros(n_nodes_3)\n",
        "\n",
        "  def run_epoch(self, X: ndarray, y: ndarray) -> float:\n",
        "    '''Return max weight update from the epoch'''\n",
        "    n_rows, n_cols = X.shape[0], X.shape[1]\n",
        "\n",
        "    # visit each row (input set)\n",
        "    max_change = 0.0\n",
        "    for row in range(n_rows):\n",
        "      t = y[row]\n",
        "      self.set_inputs(X[row, :])\n",
        "\n",
        "      self.propagate_forward()\n",
        "      max_change = max(max_change, self.propagate_backward(t))\n",
        "\n",
        "    return max_change\n",
        "\n",
        "  def propagate_forward(self):\n",
        "    self.compute_net_hidden_nodes()\n",
        "    self.compute_z_hidden_nodes()\n",
        "    self.compute_net_output_nodes()\n",
        "    self.compute_z_output_nodes()\n",
        "\n",
        "  def propagate_backward(self, t: ndarray):\n",
        "    self.compute_error_output_nodes(t)\n",
        "    self.compute_error_hidden_nodes()\n",
        "    max_change = self.update_weights_input_to_hidden()\n",
        "    max_change = max(max_change, self.update_weights_hidden_to_output())\n",
        "\n",
        "    return max_change\n",
        "\n",
        "  def compute_net_hidden_nodes(self) -> None:\n",
        "    for j in range(len(self.N_2)):\n",
        "      net = 0.0\n",
        "      for i in range(len(self.Z_1)):\n",
        "        net += self.W_1_to_2[i, j] * self.Z_1[i]\n",
        "      self.N_2[j] = net\n",
        "\n",
        "  def compute_z_hidden_nodes(self) -> None:\n",
        "    for j in range(len(self.N_2)):\n",
        "      self.Z_2[j] = self.activation(self.N_2[j])\n",
        "\n",
        "  def compute_net_output_nodes(self) -> None:\n",
        "    for j in range(len(self.N_3)):\n",
        "      net = 0.0\n",
        "      for i in range(len(self.Z_2)):\n",
        "        net += self.W_2_to_3[i, j] * self.Z_2[i]\n",
        "      self.N_3[j] = net\n",
        "\n",
        "  def compute_z_output_nodes(self) -> None:\n",
        "    for j in range(len(self.N_3)):\n",
        "      self.Z_3[j] = self.activation(self.N_3[j])\n",
        "\n",
        "  def compute_error_output_nodes(self, t: ndarray) -> None:\n",
        "    for j in range(len(self.E_3)):\n",
        "      self.E_3[j] = self.error_output_node(j, t)\n",
        "\n",
        "  def compute_error_hidden_nodes(self) -> None:\n",
        "    for j in range(len(self.E_2)):\n",
        "      self.E_2[j] = self.error_hidden_node(j)\n",
        "\n",
        "  def update_weights_input_to_hidden(self) -> float:\n",
        "    max_change = 0.0\n",
        "    for i in range(len(self.Z_1)):\n",
        "      for j in range(len(self.N_2)):\n",
        "        delta_w = self.delta_w(from_layer=1, i=i, j=j)\n",
        "        max_change = max(max_change, abs(delta_w))\n",
        "        self.W_1_to_2[i, j] += delta_w\n",
        "\n",
        "    return max_change\n",
        "\n",
        "  def update_weights_hidden_to_output(self) -> float:\n",
        "    max_change = 0.0\n",
        "    for i in range(len(self.Z_2)):\n",
        "      for j in range(len(self.N_3)):\n",
        "        delta_w = self.delta_w(from_layer=2, i=i, j=j)\n",
        "        max_change = max(max_change, abs(delta_w))\n",
        "        self.W_2_to_3[i, j] += delta_w\n",
        "\n",
        "    return max_change\n",
        "\n",
        "  def set_inputs(self, inputs: ndarray):\n",
        "    '''len(inputs) == len(self.Z_1) - 1 (last item is bias and stays 1)'''\n",
        "    for i in range(len(inputs)):\n",
        "      self.Z_1[i] = inputs[i]\n",
        "    self.Z_1[-1] = 1 # bias\n",
        "\n",
        "  def activation(self, net) -> float:\n",
        "    '''Sigmoid'''\n",
        "    return 1 / (1 + math.exp(-net))\n",
        "\n",
        "  def activation_prime(self, z: float) -> float:\n",
        "    return z * (1 - z)\n",
        "\n",
        "  def delta_w(self, from_layer: Literal[1, 2], i, j) -> float:\n",
        "    '''delta(w_ij) = C * error_j * z_i + alpha * old(delta(w_ij))'''\n",
        "    if from_layer == 1:\n",
        "      e_j = self.E_2[j]\n",
        "      z_i = self.Z_1[i]\n",
        "      old_delta_w = self.delta_W_1_to_2[i, j]\n",
        "    else: # from_layer == 2\n",
        "      e_j = self.E_3[j]\n",
        "      z_i = self.Z_2[i]\n",
        "      old_delta_w = self.delta_W_2_to_3[i, j]\n",
        "\n",
        "    delta_w = self.lr * e_j * z_i + self.momentum * old_delta_w\n",
        "\n",
        "    if from_layer == 1:\n",
        "      self.delta_W_1_to_2[i, j] = delta_w\n",
        "    else: # from_layer == 2\n",
        "      self.delta_W_2_to_3[i, j] = delta_w\n",
        "\n",
        "    return delta_w\n",
        "\n",
        "  def error_output_node(self, j, t: ndarray) -> float:\n",
        "    '''(t_j - z_j) * f'(net_j)'''\n",
        "    z_j = self.Z_3[j]\n",
        "\n",
        "    return (t[j] - z_j) * self.activation_prime(z_j)\n",
        "\n",
        "  def error_hidden_node(self, j) -> float:\n",
        "    '''sum_k(error_k * w_jk) * f'(net_j)'''\n",
        "    sum = 0.0\n",
        "    for k in range(len(self.E_3)):\n",
        "      sum += self.E_3[k] * self.W_2_to_3[j, k]\n",
        "\n",
        "    return sum * self.activation_prime(self.Z_2[j])\n",
        "\n",
        "  def print_weights(self):\n",
        "    print(\"weights from input to hidden:\")\n",
        "    print(self.W_1_to_2)\n",
        "    print(\"weights from hidden to output:\")\n",
        "    print(self.W_2_to_3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " Test with problem from class example (first input set) and Backprop HW (second input set)\n",
        " initial_weights: 0.5, lr: 1.0, momentum: 0, shuffle=False, hidden_layer_widths: 2\n",
        " set1: .9, .6 -> 0\n",
        " set2: -1, .4 -> .2\n",
        "\n",
        " expected weights (AFTER 1 EPOCH):\n",
        "    row=from, col=to\n",
        "        4          5\n",
        "    1 | 0.49990301 0.49990301\n",
        "    2 | 0.48896161 0.48896161\n",
        "    3 | 0.4781734  0.4781734\n",
        "\n",
        "        7\n",
        "    4 | 0.33884203\n",
        "    5 | 0.33884203\n",
        "    6 | 0.26186419\n",
        "\"\"\"\n",
        "X = np.array([[.9, .6], [-1, .4]])\n",
        "y = np.array([[0], [.2]])\n",
        "clf = MLP(lr=1, momentum=0, shuffle=False, hidden_layer_widths=2)\n",
        "clf.fit(X, y, initial_weights=0.5, max_iter=1)\n",
        "\n",
        "print(\"Results Should Be Same As MLP HW:\")\n",
        "clf.print_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBwN_9s8Jrrm",
        "outputId": "39732f69-7dae-4f57-d0fe-a6d6fefecbba"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results Should Be Same As MLP HW:\n",
            "weights from input to hidden:\n",
            "[[0.49990301 0.49990301]\n",
            " [0.48896161 0.48896161]\n",
            " [0.4781734  0.4781734 ]]\n",
            "weights from hidden to output:\n",
            "[[0.33884203]\n",
            " [0.33884203]\n",
            " [0.26186419]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the HW problem but w/1000 iterations\n",
        "X = np.array([[.9, .6], [-1, .4]])\n",
        "y = np.array([[0], [.2]])\n",
        "clf = MLP(lr=1, momentum=0, shuffle=True, hidden_layer_widths=2)\n",
        "clf.fit(X, y, max_iter=1000)\n",
        "\n",
        "print(\"Num epochs: \", str(clf.n_epochs))\n",
        "print(\"\\nOutput Predictions w/MLP HW Problem (Expected: 0, 0.2):\")\n",
        "print(clf.predict(X))\n",
        "\n",
        "print(\"\\nScore:\")\n",
        "print(clf.score(X, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXIlNuDvwtS0",
        "outputId": "c99912fa-0e94-4ca5-8880-538b41b683b8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num epochs:  936\n",
            "\n",
            "Output Predictions w/MLP HW Problem (Expected: 0, 0.2):\n",
            "[[0.02925073]\n",
            " [0.19439183]]\n",
            "\n",
            "Score:\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iris Dataset\n",
        "X, y = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "clf = MLP(lr=.1, momentum=.1, shuffle=True, hidden_layer_widths=8)\n",
        "clf.fit(X_train, y_train, max_iter=1000)\n",
        "print(\"Results from Iris Dataset\")\n",
        "print(\"Num epochs: \", str(clf.n_epochs))\n",
        "\n",
        "print(\"\\nTraining Set Score:\")\n",
        "print(clf.score(X_train, y_train))\n",
        "print(\"\\nTest Set Score:\")\n",
        "print(clf.score(X_test, y_test))\n",
        "\n",
        "print(\"\\n20 Sample Predictions on Iris Dataset:\")\n",
        "actual = clf.predict(X_test)\n",
        "expected = y_test\n",
        "for row in range(actual.shape[0]):\n",
        "  print(\"expected: \", str(expected[row, :]), \", actual: \", str(actual[row, :]))\n",
        "  if row > 20:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8iB9Q4lQI1P",
        "outputId": "235b5ca0-ecc7-46f4-b36a-83b29b12e408"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results from Iris Dataset\n",
            "Num epochs:  1000\n",
            "\n",
            "Training Set Score:\n",
            "0.9916666666666667\n",
            "\n",
            "Test Set Score:\n",
            "0.9333333333333333\n",
            "\n",
            "20 Sample Predictions on Iris Dataset:\n",
            "expected:  [0 0 1] , actual:  [1.53153325e-05 1.40178076e-02 9.91817621e-01]\n",
            "expected:  [0 0 1] , actual:  [1.48706319e-05 1.35087872e-02 9.92330210e-01]\n",
            "expected:  [0 1 0] , actual:  [1.08503814e-02 9.98938918e-01 2.69593324e-04]\n",
            "expected:  [0 0 1] , actual:  [2.55370828e-05 2.96982524e-02 9.78784045e-01]\n",
            "expected:  [0 0 1] , actual:  [1.72051931e-05 1.62100528e-02 9.89809557e-01]\n",
            "expected:  [0 0 1] , actual:  [4.04966292e-05 6.64816391e-02 9.49621515e-01]\n",
            "expected:  [0 1 0] , actual:  [1.22758246e-04 3.35826650e-01 6.60328353e-01]\n",
            "expected:  [0 1 0] , actual:  [0.00490364 0.99680901 0.00129723]\n",
            "expected:  [0 1 0] , actual:  [1.35394473e-02 9.99183425e-01 1.61051826e-04]\n",
            "expected:  [0 1 0] , actual:  [1.73940664e-02 9.99015284e-01 1.42695513e-04]\n",
            "expected:  [1 0 0] , actual:  [9.91112797e-01 1.36648722e-02 1.70857807e-05]\n",
            "expected:  [1 0 0] , actual:  [9.92746522e-01 1.18624808e-02 1.71730790e-05]\n",
            "expected:  [0 1 0] , actual:  [1.30797648e-02 9.99201765e-01 1.64903548e-04]\n",
            "expected:  [0 1 0] , actual:  [1.18278205e-02 9.99117497e-01 2.05905743e-04]\n",
            "expected:  [0 1 0] , actual:  [7.11880597e-03 9.98498656e-01 5.36893890e-04]\n",
            "expected:  [1 0 0] , actual:  [9.91426664e-01 1.33303806e-02 1.71032226e-05]\n",
            "expected:  [0 1 0] , actual:  [0.00187348 0.98604571 0.00796794]\n",
            "expected:  [1 0 0] , actual:  [9.91827667e-01 1.27943236e-02 1.70853872e-05]\n",
            "expected:  [0 1 0] , actual:  [6.96809845e-03 9.98294076e-01 6.26877018e-04]\n",
            "expected:  [1 0 0] , actual:  [9.89126795e-01 1.62616761e-02 1.71641630e-05]\n",
            "expected:  [1 0 0] , actual:  [9.91716118e-01 1.29171053e-02 1.70819517e-05]\n",
            "expected:  [0 1 0] , actual:  [0.00515737 0.99750161 0.00100907]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2.7.16 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "2.7.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}