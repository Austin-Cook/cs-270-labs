{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# K-Nearest Neighbor Lab\n",
        "Read over the sklearn info on [nearest neighbor learners](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple\n",
        "from typing import List\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "import csv\n",
        "import math\n",
        "from enum import Enum\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "np.set_printoptions(threshold=10000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(X: ndarray) -> ndarray:\n",
        "  return MinMaxScaler().fit_transform(X)\n",
        "\n",
        "def normalize_continuous_features(X: ndarray, nominal_feature_indexes: set):\n",
        "  for i in range(X.shape[1]):\n",
        "    if i not in nominal_feature_indexes:\n",
        "      X[:, i:i+1] = normalize(X[:, i:i+1])\n",
        "\n",
        "  return X\n",
        "\n",
        "def load_glass(flatten=True) -> Tuple[ndarray, ndarray]:\n",
        "  \"\"\"\n",
        "  From: https://archive.ics.uci.edu/dataset/42/glass+identification\n",
        "  Purpose: Predict glass type\n",
        "\n",
        "  Features:\n",
        "    RI\trefractive inde (REAL)\n",
        "    Na\tPercent Sodium (REAL)\n",
        "    Mg\tPercent Magnesium (REAL)\n",
        "    Al\tPercent Aluminum (REAL)\n",
        "    Si\tPercent Silicon (REAL)\n",
        "    K\t  Percent Potassium (REAL)\n",
        "    Ca\tPercent Calcium (REAL)\n",
        "    Ba\tPercent Barium (REAL)\n",
        "    Fe\tPercent Iron (REAL)\n",
        "\n",
        "  Classification:\n",
        "    Type of glass (CATEGORICAL)\n",
        "  \"\"\"\n",
        "  # load csv file\n",
        "  with open('glass.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "  # convert to numby array, and extract X and y\n",
        "  data = np.array(data)\n",
        "  X = data[:, 1:-1]\n",
        "  y = data[:, -1:]\n",
        "  X = X.astype(float)\n",
        "  y = y.astype(float)\n",
        "  if flatten:\n",
        "    y = y.flatten()\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def load_magic_telescope() -> Tuple[ndarray, ndarray]:\n",
        "  \"\"\"\n",
        "  Purpose: Classification as gamma or hardon from telestope data\n",
        "\n",
        "  Features:\n",
        "    fLength (REAL)\n",
        "    fWidth (REAL)\n",
        "    fSize (REAL)\n",
        "    fConc (REAL)\n",
        "    fConc1 (REAL)\n",
        "    fAsym (REAL)\n",
        "    fM3Long (REAL)\n",
        "    fM3Trans (REAL)\n",
        "    fAlpha (REAL)\n",
        "    fDist (REAL)\n",
        "\n",
        "  Classifications:\n",
        "    -> {g, h} (gamma (signal), hadron (background))\n",
        "  \"\"\"\n",
        "  data_arff = arff.loadarff(\"MagicTelescope.arff\")\n",
        "  data_df = pd.DataFrame(data_arff[0])\n",
        "  data_np = data_df.to_numpy()\n",
        "  X = data_np[:, :-1]\n",
        "  y = data_np[:, -1]\n",
        "\n",
        "  # convert strings/booleans to numbers\n",
        "  X = X.astype(float)\n",
        "  y = np.where(y == b'g', 'g', 'h')\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def load_housing() -> Tuple[ndarray, ndarray]:\n",
        "  \"\"\"\n",
        "  Purpose: Regression of home value in $1000's\n",
        "\n",
        "  Features:\n",
        "    CRIM    (REAL) per capita crime rate by town\n",
        "    ZN\t\t  (REAL) proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "    INDUS\t  (REAL) proportion of non-retail business acres per town\n",
        "    CHAS\t\t{0,1}  Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "    NOX\t  \t(REAL) nitric oxides concentration (parts per 10 million)\n",
        "    RM\t\t  (REAL) average number of rooms per dwelling\n",
        "    AGE\t\t  (REAL) proportion of owner-occupied units built prior to 1940\n",
        "    DIS\t  \t(REAL) weighted distances to five Boston employment centres\n",
        "    RAD\t\t  (REAL) index of accessibility to radial highways\n",
        "    TAX\t\t  (REAL) full-value property-tax rate per $10,000\n",
        "    PTRATIO\t(REAL) pupil-teacher ratio by town\n",
        "    B\t\t    (REAL) 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "    LSTAT\t  (REAL) % lower status of the population\n",
        "\n",
        "  Regression:\n",
        "    -> MEDV (REAL) Median value of owner-occupied homes in $1000's\n",
        "  \"\"\"\n",
        "  data_arff = arff.loadarff(\"housing.arff\")\n",
        "  data_df = pd.DataFrame(data_arff[0])\n",
        "  data_np = data_df.to_numpy()\n",
        "  X = data_np[:, :-1]\n",
        "  y = data_np[:, -1]\n",
        "\n",
        "  # convert strings/booleans to numbers\n",
        "  X = X.astype(float)\n",
        "  y = y.astype(float)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def load_lymph() -> Tuple[ndarray, ndarray]:\n",
        "  \"\"\"\n",
        "  Purpose: Classification of lymphography\n",
        "\n",
        "  Features:\n",
        "    'lymphatics'      { normal, arched, deformed, displaced}\n",
        "    'block_of_affere' { no, yes}\n",
        "    'bl_of_lymph_c'   { no, yes}\n",
        "    'bl_of_lymph_s'   { no, yes}\n",
        "    'by_pass'         { no, yes}\n",
        "    'extravasates'    { no, yes}\n",
        "    'regeneration_of' { no, yes}\n",
        "    'early_uptake_in' { no, yes}\n",
        "    'lym_nodes_dimin' integer\n",
        "    'lym_nodes_enlar' integer\n",
        "    'changes_in_lym'  { bean, oval, round}\n",
        "    'defect_in_node'  { no, lacunar, lac_margin, lac_central}\n",
        "    'changes_in_node' { no, lacunar, lac_margin, lac_central}\n",
        "    'changes_in_stru' { no, grainy, drop_like, coarse, diluted, reticular, stripped, faint}\n",
        "    'special_forms'   { no, chalices, vesicles}\n",
        "    'dislocation_of'  { no, yes}\n",
        "    'exclusion_of_no' { no, yes}\n",
        "    'no_of_nodes_in'  integer\n",
        "\n",
        "  Classifications:\n",
        "    -> { normal, metastases, malign_lymph, fibrosis }\n",
        "  \"\"\"\n",
        "  data_arff = arff.loadarff(\"lymph.arff\")\n",
        "  data_df = pd.DataFrame(data_arff[0])\n",
        "  data_np = data_df.to_numpy()\n",
        "  X = data_np[:, :-1]\n",
        "  y = data_np[:, -1]\n",
        "\n",
        "  # # convert strings/booleans to numbers\n",
        "  # X = X.astype(float)\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "90U5ciQsoFnV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co3J8AHYCKjA"
      },
      "source": [
        "## 1 K-Nearest Neighbor (KNN) algorithm\n",
        "\n",
        "### 1.1 (15%) Basic KNN Classification\n",
        "\n",
        "Learn the [Glass data set](https://archive.ics.uci.edu/dataset/42/glass+identification) using [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) with default parameters.\n",
        "- Randomly split your data into train/test.  Anytime we don't tell you specifics (such as what percentage is train vs test) choose your own reasonable values\n",
        "- Give typical train and test set accuracies after running with different random splits\n",
        "- Print the output probabilities for a test set (predict_proba)\n",
        "- Try it with different p values (Minkowskian exponent) and discuss any differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fapyGDkjCKjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdd2b70-d43a-4612-cbd8-1eceb79a6076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p:  0.5\n",
            "   train score:  0.783625730994152\n",
            "   test score:  0.7674418604651163\n",
            "   predict_proba: [[0.  1.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.  0.8 0.  0.  0.  0.2]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.4 0.  0.2 0.2 0.2]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  0.4 0.  0.6 0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.2 0.  0.8 0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.  0.6 0.  0.4 0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.  0.2 0.  0.  0.2 0.6]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.6 0.2 0.  0.  0.  0.2]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]]\n",
            "p:  1\n",
            "   train score:  0.7602339181286549\n",
            "   test score:  0.7209302325581395\n",
            "   predict_proba: [[0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.  0.8 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.2 0.  0.  0.  0.8]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.4 0.2 0.4 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.2 0.2 0.4 0.  0.  0.2]\n",
            " [0.2 0.2 0.4 0.  0.  0.2]\n",
            " [0.  0.6 0.  0.4 0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.2 0.  0.8]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.8 0.2 0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.4 0.  0.  0.6 0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.2 0.2 0.  0.  0.6 0. ]\n",
            " [0.  0.4 0.  0.  0.6 0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]]\n",
            "p:  1.5\n",
            "   train score:  0.7719298245614035\n",
            "   test score:  0.627906976744186\n",
            "   predict_proba: [[0.  0.  0.  0.  0.  1. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.  0.2 0.  0.  0.8 0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.2 0.  0.8]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  1.  0.  0. ]\n",
            " [0.  0.2 0.  0.4 0.4 0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.4 0.2 0.4 0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.4 0.4 0.  0.  0. ]\n",
            " [0.  0.  0.  0.2 0.  0.8]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.2 0.  0.  0.  0.4 0.4]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.6 0.  0.2 0.2 0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]]\n",
            "p:  2\n",
            "   train score:  0.7660818713450293\n",
            "   test score:  0.7209302325581395\n",
            "   predict_proba: [[0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.2 0.8]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.6 0.  0.4 0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.2 0.6 0.  0.  0.2]\n",
            " [0.  0.4 0.  0.  0.6 0. ]\n",
            " [0.  0.2 0.  0.6 0.  0.2]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  0.2 0.  0.  0.4 0.4]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]]\n",
            "p:  2.5\n",
            "   train score:  0.7426900584795322\n",
            "   test score:  0.8604651162790697\n",
            "   predict_proba: [[0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.6 0.  0.2 0.  0.2]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.2 0.2 0.  0.6 0.  0. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.2 0.  0.4 0.  0.4]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.4 0.  0.6 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.2 0.  0.4 0.4 0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.8 0.  0.2 0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]]\n",
            "p:  5\n",
            "   train score:  0.7660818713450293\n",
            "   test score:  0.6511627906976745\n",
            "   predict_proba: [[0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.  0.6 0.  0.4 0.  0. ]\n",
            " [0.6 0.  0.4 0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.8 0.2 0.  0.  0.  0. ]\n",
            " [0.2 0.4 0.  0.2 0.  0.2]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.4 0.  0.6 0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.8 0.  0.2 0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.4 0.  0.6]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  0.4 0.  0.6 0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.4 0.2 0.  0.  0.2 0.2]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  0.4 0.  0.  0.6 0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.2 0.8 0.  0.  0.  0. ]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.4 0.6 0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.  0.4 0.  0.4 0.2 0. ]\n",
            " [1.  0.  0.  0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]]\n",
            "p:  1000000\n",
            "   train score:  0.5555555555555556\n",
            "   test score:  0.4883720930232558\n",
            "   predict_proba: [[0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.4 0.  0.6 0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.6 0.2 0.  0.  0.  0.2]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.6 0.4 0.  0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.6 0.2 0.2 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  1. ]\n",
            " [0.4 0.4 0.2 0.  0.  0. ]\n",
            " [0.4 0.4 0.  0.  0.  0.2]\n",
            " [0.2 0.6 0.2 0.  0.  0. ]\n",
            " [0.  1.  0.  0.  0.  0. ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py:653: UserWarning: Mind that for 0 < p < 1, Minkowski metrics are not distance metrics. Continuing the execution with `algorithm='brute'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Learn the glass data\n",
        "\n",
        "p value:\n",
        "- Used when metric='minkowski' (default)\n",
        "- Values:\n",
        "  p < 1 (Closer distances are more important)\n",
        "  p = 1 (Manhattan Distance)\n",
        "  p = 2 (Euclidean Distance)\n",
        "  p > 2 (Farther distances are more important)\n",
        "\"\"\"\n",
        "X, y = load_glass()\n",
        "\n",
        "p_values = [0.5, 1, 1.5, 2, 2.5, 5, 1000000]\n",
        "for p in p_values:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  knn = KNeighborsClassifier(p=p)\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  print(\"p: \", str(p))\n",
        "  print(\"   train score: \", str(knn.score(X_train, y_train)))\n",
        "  print(\"   test score: \", str(knn.score(X_test, y_test)))\n",
        "  print(\"   predict_proba:\" , str(knn.predict_proba(X_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ykz-7ACKjA"
      },
      "source": [
        "Discussion\n",
        "\n",
        "The results for this trial were quite interesting. The p value is used when the metric hyperparameter is set to 'minkowski', and since this is the default, I didn't have to set it in my model. The p value sets how sensitive the metric is to higher differences in distance. When p is less then 1, it is more sensitive to closer distances. When it is 1, it uses manhattan distance, which can be thought of as the distance to travel somewhere when you can only take street blocks. When p is 2, euclidean distance is used which is \"as the crow flies\". As it is increased past 2, farther distances (points farther away) become more important than they otherwise would be.\n",
        "\n",
        "This makes sense to me becuase when you compare Manhattan  to Euclidian distance, Manhattan would yield a further distance for something far away because you can't cut straight to the other point. And if you increased past 2 (Euclidian) it would continue to make the distance less and less for something that is actually far away.\n",
        "\n",
        "In my trials, it was interesting to find a sweet spot. Of course, if I had done more trials at each p value, I'm sure my results would be a little more accurate, simple because the luck of the train test split is playing a role here. The sweet spot was p=2.5 which is a little past Euclidian distance, and it yielded 86.04% accuracy. Past that, the results dropped. Before 2.5, there was quite a bit of fluctuation, though the next highest was 76.74% for the test set score. It's also interesting that the training set score was pretty constant at around 76% until I increased p to a very high value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2 KNN Classification with normalization and distance weighting\n",
        "\n",
        "Use the [magic telescope](https://axon.cs.byu.edu/data/uci_class/MagicTelescope.arff) dataset\n",
        "\n",
        "### 2.1 (5%) - Without Normalization or Distance Weighting\n",
        "- Do random 80/20 train/test splits each time\n",
        "- Run with k=3 and *without* distance weighting and *without* normalization\n",
        "- Show train and test set accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSoasDQSKXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327846f3-d75b-4ecc-8304-dce88c98c0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial:\n",
            "   train score:  0.8903128286014721\n",
            "   test score:  0.7823343848580442\n",
            "Trial:\n",
            "   train score:  0.8858438485804416\n",
            "   test score:  0.7957413249211357\n",
            "Trial:\n",
            "   train score:  0.8851866456361724\n",
            "   test score:  0.7941640378548895\n",
            "Trial:\n",
            "   train score:  0.8870268138801262\n",
            "   test score:  0.8070452155625657\n",
            "Trial:\n",
            "   train score:  0.8860410094637224\n",
            "   test score:  0.805993690851735\n",
            "Trial:\n",
            "   train score:  0.8865667718191378\n",
            "   test score:  0.7960042060988434\n",
            "Trial:\n",
            "   train score:  0.8826235541535226\n",
            "   test score:  0.8065194532071503\n",
            "Trial:\n",
            "   train score:  0.8845951629863302\n",
            "   test score:  0.7981072555205048\n",
            "Trial:\n",
            "   train score:  0.8848580441640379\n",
            "   test score:  0.7986330178759201\n",
            "Trial:\n",
            "   train score:  0.8881440588853838\n",
            "   test score:  0.7807570977917981\n",
            "\n",
            "Averages:\n",
            "   avg train score:  0.8861198738170348\n",
            "   avg test score:  0.7965299684542588\n"
          ]
        }
      ],
      "source": [
        "# Learn magic telescope data\n",
        "# WOUT/Normalization, WOUT/Distance Weighting\n",
        "X, y = load_magic_telescope()\n",
        "\n",
        "avg_train_acc = 0.0\n",
        "avg_test_acc = 0.0\n",
        "n_trials = 10\n",
        "for i in range(n_trials):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  knn = KNeighborsClassifier(n_neighbors=3, weights='uniform') # NOTE - uniform is default\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  train_acc = knn.score(X_train, y_train)\n",
        "  test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  print(\"Trial:\")\n",
        "  print(\"   train score: \", str(train_acc))\n",
        "  print(\"   test score: \", str(test_acc))\n",
        "\n",
        "print(\"\\nAverages:\")\n",
        "print(\"   avg train score: \", str(avg_train_acc / n_trials))\n",
        "print(\"   avg test score: \", str(avg_test_acc / n_trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKEQHszpCKjB"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "In each of the trials, the test accuracy was about 9% lower than the training score, which seems realistic. Here, I did not use normalization. Normalization puts each of the values on the same scale (typically between 0 and 1), and can help results be found more quickly and accurately, and provides overall more standardization to the data.\n",
        "\n",
        "The other thing to note is that distance weighting was not used. Distance weighting is an add on to plain KNN where points that are closer to the point q have a higher weight. The weights give each point a vote, and the classification with the highest overall vote from the points that are in the k-nearest neighbors is the classification that is assigned to the value. The result of not using distance weighing is that all points have the same weight (1) even if one point is quite far away from the point itself.\n",
        "\n",
        "Based on this, my prediction is that the results will be more accurate in the next section where we do use distance weighting (and also to some extent from normalization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twB7j_b9CKjB"
      },
      "source": [
        "### 2.2 (10%) With Normalization\n",
        "- Try it with k=3 without distance weighting but *with* normalization of input features.  You may use any reasonable normalization approach (e.g. standard min-max normalization between 0-1, z-transform, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f-IdNtKCKjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa70353-c100-4ec0-816b-8f6d5135a378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial:\n",
            "   train score:  0.898593585699264\n",
            "   test score:  0.8338590956887487\n",
            "Trial:\n",
            "   train score:  0.9014195583596214\n",
            "   test score:  0.8349106203995794\n",
            "Trial:\n",
            "   train score:  0.9008280757097792\n",
            "   test score:  0.830441640378549\n",
            "Trial:\n",
            "   train score:  0.9010252365930599\n",
            "   test score:  0.8286014721345951\n",
            "Trial:\n",
            "   train score:  0.9024053627760252\n",
            "   test score:  0.8301787592008412\n",
            "Trial:\n",
            "   train score:  0.9015509989484752\n",
            "   test score:  0.8314931650893796\n",
            "Trial:\n",
            "   train score:  0.8999079915878023\n",
            "   test score:  0.844111461619348\n",
            "Trial:\n",
            "   train score:  0.8996451104100947\n",
            "   test score:  0.8378023133543638\n",
            "Trial:\n",
            "   train score:  0.8994479495268138\n",
            "   test score:  0.833596214511041\n",
            "Trial:\n",
            "   train score:  0.9000394321766562\n",
            "   test score:  0.8259726603575184\n",
            "\n",
            "Averages:\n",
            "   avg train score:  0.9004863301787592\n",
            "   avg test score:  0.8330967402733964\n"
          ]
        }
      ],
      "source": [
        "# Train/Predict with normalization\n",
        "# W/Normalization, WOUT/Distance Weighting\n",
        "X, y = load_magic_telescope()\n",
        "X = normalize(X)\n",
        "\n",
        "avg_train_acc = 0.0\n",
        "avg_test_acc = 0.0\n",
        "n_trials = 10\n",
        "for i in range(n_trials):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  knn = KNeighborsClassifier(n_neighbors=3, weights='uniform') # NOTE - uniform is default\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  train_acc = knn.score(X_train, y_train)\n",
        "  test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  print(\"Trial:\")\n",
        "  print(\"   train score: \", str(train_acc))\n",
        "  print(\"   test score: \", str(test_acc))\n",
        "\n",
        "print(\"\\nAverages:\")\n",
        "print(\"   avg train score: \", str(avg_train_acc / n_trials))\n",
        "print(\"   avg test score: \", str(avg_test_acc / n_trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O1VUr0hCKjB"
      },
      "source": [
        "*Discuss the results of using normalized data vs. unnormalized data*\n",
        "\n",
        "As predicted, using normalized data did help a little bit. Both test and training set accuracies improved. Test set accuracy improved by about 1.5%, while training set accuracy improved by about 3%. While the difference isn't huge, it certainly makes it worth using. From what I've seen with using normalization here as well as with other algorithms in other labs, normalization provides a little more stability to the data.\n",
        "\n",
        "For example, if most features have values between 5 and 10, while one feature has values between 5,000 and 10,000, the feature with larger value may skew the results from the other features. While many algorithms are able to compensate for this, it typically takes more time and often doesn't work perfectly. Therefore, normalizing the data is important to avoid these issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLZLv2bECKjC"
      },
      "source": [
        "### 2.3 (10%) With Distance Weighting\n",
        "- Try it with k=3 and with distance weighting *and* normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjQTt1PUCKjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43534e4-d0d1-4d25-c1b5-c61a01964086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8427970557308097\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.830441640378549\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8288643533123028\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8351735015772871\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8259726603575184\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8280757097791798\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8293901156677181\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8291272344900105\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.8359621451104101\n",
            "Trial:\n",
            "   train score:  1.0\n",
            "   test score:  0.832807570977918\n",
            "\n",
            "Averages:\n",
            "   avg train score:  1.0\n",
            "   avg test score:  0.8318611987381702\n"
          ]
        }
      ],
      "source": [
        "#Train/Precdict with normalization and distance weighting\n",
        "# W/Normalization, W/Distance Weighting\n",
        "X, y = load_magic_telescope()\n",
        "X = normalize(X)\n",
        "\n",
        "avg_train_acc = 0.0\n",
        "avg_test_acc = 0.0\n",
        "n_trials = 10\n",
        "for i in range(n_trials):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  train_acc = knn.score(X_train, y_train)\n",
        "  test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  print(\"Trial:\")\n",
        "  print(\"   train score: \", str(train_acc))\n",
        "  print(\"   test score: \", str(test_acc))\n",
        "\n",
        "print(\"\\nAverages:\")\n",
        "print(\"   avg train score: \", str(avg_train_acc / n_trials))\n",
        "print(\"   avg test score: \", str(avg_test_acc / n_trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1JGsg4GCKjC"
      },
      "source": [
        "Comparison and Discussion\n",
        "\n",
        "The default value for 'weights' in KNeighborsClassifier is 'uniform' which means it doesn't use distance weighting. Here, I change it to 'distance' which turns it on.\n",
        "\n",
        "When distance weighting is used, the points that are further away from q have smaller weights. Voting occurs where each point gets a voting weight depending on its distance, and the classificaiton with the highest total vote is the classification that is assigned to q.\n",
        "\n",
        "When looking at the results, the first thing to noticeis that the training accuracy was one for every trial. This makes sense, becuase the algorithm is weighting closer points higher. Since each point that we are testing was also used for training, our q will lie exactly on top of the cooreponding point used for training. Because of this, its weight will be very high, if not infinite, thus meaning that the classification for q will alway end up being the same as itself in from the when it was used in training. No other point could realistically overpower it.\n",
        "\n",
        "Now, take a look at the test accuracies. It is interetsing to note that there was no improvement in test accuracy. While I was surprised at this, I think I understand the reasoning after thinking about it. I believe the problem is due to slight overfit. We are only using 3 as the k-value, which is a pretty low number. Since we are using distance weighting, we are much more likely to just take the value of the closest points even if they are just noise. We aren't letting the point look at enough general information. Not also that the accuracy didn't drop a significant amount. I think it is alright to keep using distance weighting, but what will really allow for better generalization here will be increasing k, which I will experiment with in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G2vk6hzCKjC"
      },
      "source": [
        "### 2.4 (10%) Different k Values\n",
        "- Using your normalized data with distance weighting, create one graph with classification accuracy on the test set on the y-axis and k values on the x-axis.\n",
        "- Use values of k from 1 to 15.  Use the same train/test split for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbeq5WqSCKjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6f16b6-6253-45bb-cac3-7d54c2fb735d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 1         2         3         4         5         6         7         8       9        10        11        12        13        14        15\n",
            "--------  --------  --------  --------  --------  --------  --------  --------  --------  ------  --------  --------  --------  --------  --------  --------\n",
            "Test acc  0.804942  0.804942  0.831756  0.828076  0.845163  0.844111  0.845163  0.845952  0.8449  0.845426  0.844374  0.845689  0.845163  0.842797  0.845689\n"
          ]
        }
      ],
      "source": [
        "# Calculate and Graph classification accuracy vs k values\n",
        "# W/Normalization, W/Distance Weighting\n",
        "X, y = load_magic_telescope()\n",
        "X = normalize(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "headers = [i if i > 0 else \"\" for i in range(16)]\n",
        "table = [[\"\" if i > 0 else \"Test acc\" for i in range(16)]]\n",
        "\n",
        "for k in range(1, 16):\n",
        "  knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "  table[0][k] = test_acc\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDGIIpR3CKjC"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "Its very interesting to see how quite a bit of improvement was achieved from bumping up the k-value a few, then progress quickly stopped. At k=1 and 2, the results were identical. At 3, there was some improvement, at 4, results were slightly worse than three, and then at 5, results essentially peaked. Its important to note that at k=8, we did see the highest accuracy, though it was only by about 0.0005, and after that there was no more improvement.\n",
        "\n",
        "I am strongly convinced that the value of k should depend on the shape of the data and how the test/real world data will vary from the training data. If data needs to be strongly generalized, it would be quite important to use a higher k value. However, if you know that the data is very rigid, fits specific patterns in a set way, and doesn't vary much from that, it may be better to use a smaller k. However, I believe that it is important to do a test like what I did above to experiment with different numbers for k and see which one actually yields the best results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "## 3 KNN Regression with normalization and distance weighting\n",
        "\n",
        "Use the [sklean KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) on the [housing price prediction](https://axon.cs.byu.edu/data/uci_regression/housing.arff) problem.  \n",
        "### 3.1 (5%) Ethical Data\n",
        "Note this data set has an example of an inappropriate input feature which we discussed.  State which feature is inappropriate and discuss why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndEnhx-oCKjC"
      },
      "source": [
        "*Discuss the innapropriate feature*\n",
        "\n",
        "The inappropriate feature in the dataset is `B` which is defined by the file as `1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town`. This feature is inappropriate for several reasons. For one, it can be used to discriminate. We don't want to predict house prices as being lower simply because more people of a single race live in one geographical location.\n",
        "\n",
        "Additionally, it may not always be correct. While many populations that have a high percentage of black people may tend to have lower house prices, that is not always the case. Sometimes, it is exactly the opposite of that. If we use this feature, we may inappropriately lower the selling cost/evaluation of homes in high value areas.\n",
        "\n",
        "In general, it is important to avoid having racial factors as data features, because by nature, it is generalizing a race even though it may not be fairly representative. Beyond this, it can be offensive and rude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGSvYEMrCKjC"
      },
      "source": [
        "### 3.2 (15%) - KNN Regression\n",
        "- Do random 80/20 train/test splits each time\n",
        "- Run with k=3\n",
        "- Print the score (coefficient of determination) and Mean Absolute Error (MAE) for the train and test set for the cases of\n",
        "  - No input normalization and no distance weighting\n",
        "  - Normalization and no distance weighting\n",
        "  - Normalization and distance weighting\n",
        "- Normalize inputs features where needed but do not normalize the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBGUn43ASiXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30f9a48-efa8-4f1f-e1e0-1fbcc28f4ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial                                               Train Score    Train MAE    Test Score    Test MAE\n",
            "------------------------------------------------  -------------  -----------  ------------  ----------\n",
            "No input normalization and no distance weighting       0.793194      2.85759      0.434714     4.56928\n",
            "Normalization and no distance weighting                0.892709      2.03399      0.812289     2.79542\n",
            "Normalization and distance weighting                   1             0            0.824403     2.63068\n"
          ]
        }
      ],
      "source": [
        "# Learn and experiment with housing price prediction data\n",
        "# Score: Coefficient of determination\n",
        "headers = [\"Trial\", \"Train Score\", \"Train MAE\", \"Test Score\", \"Test MAE\"]\n",
        "table = []\n",
        "\n",
        "trials = [\n",
        "    {'normalize': False, 'weights': 'uniform', 'title': 'No input normalization and no distance weighting'},\n",
        "    {'normalize': True, 'weights': 'uniform', 'title': 'Normalization and no distance weighting'},\n",
        "    {'normalize': True, 'weights': 'distance', 'title': 'Normalization and distance weighting'}\n",
        "]\n",
        "\n",
        "for trial in trials:\n",
        "  X, y = load_housing()\n",
        "\n",
        "  if trial['normalize']:\n",
        "    X = normalize(X)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  knn = KNeighborsRegressor(n_neighbors=3, weights=trial['weights'])\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  train_score = knn.score(X_train, y_train)\n",
        "  test_score = knn.score(X_test, y_test)\n",
        "  train_mae = mean_absolute_error(knn.predict(X_train), y_train)\n",
        "  test_mae = mean_absolute_error(knn.predict(X_test), y_test)\n",
        "\n",
        "  table.append([trial['title'], train_score, train_mae, test_score, test_mae])\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aMsxsRZCKjC"
      },
      "source": [
        "*Discuss your results*\n",
        "\n",
        "Here I perform 3 trials using the KNeighborsRegressor. As expected, results were best with normalization and distance weighting. As I explained above, the training score is 1, becasue distance weighting is used, and the weight for a point will be infinite or very high if it lines up with the point that is being scored. For the same reason, its mean absolute error (average absolute value of difference between actual and predicted) is 0.\n",
        "\n",
        "Something that is interesting to note is that from the 2nd to 3rd trial, the MAE dropped by a more noticeable amount than the score increased when doing the same test with classification. As I explained above, I imagine it simple is due to the nuances in the dataset itself, and some improvement should be expected in cases where generalization is more important.\n",
        "\n",
        "When you look at the dataset itself, the regression value ranges from about 8 to 50. When you compare the MAE values to this, they seem to be realistic. For example, on the 3rd trial, the MAE was 2.63. 2.63/50 = 5.26% so it appears to be within a threshold that is realistic. Its important to note that the score function returns the coefficient of determination, which is more complicated than the MAE. That is why the score wasn't just 1 - MAE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icub_X9PCKjC"
      },
      "source": [
        "### 3.3 (10%)  Different k Values\n",
        "- Using housing with normalized data and distance weighting, create one graph with MAE on the test set on the y-axis and k values on the x-axis\n",
        "- Use values of k from 1 to 15.  Use the same train/test split for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9psUxFOCKjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1add7563-02ce-4cfe-ee8b-198cef41cb1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                1        2        3        4        5        6       7        8        9       10       11       12       13       14       15\n",
            "--------  -------  -------  -------  -------  -------  -------  ------  -------  -------  -------  -------  -------  -------  -------  -------\n",
            "Test MAE  3.12451  2.68805  2.81457  2.94946  3.02369  3.09449  3.2244  3.35564  3.39624  3.42056  3.45665  3.48168  3.48995  3.52985  3.51601\n"
          ]
        }
      ],
      "source": [
        "# Learn and graph for different k values\n",
        "X, y = load_housing()\n",
        "X = normalize(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "headers = [i if i > 0 else \"\" for i in range(16)]\n",
        "table = [[\"\" if i > 0 else \"Test MAE\" for i in range(16)]]\n",
        "\n",
        "for k in range(1, 16):\n",
        "  knn = KNeighborsRegressor(n_neighbors=k, weights='distance')\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  test_mae = mean_absolute_error(knn.predict(X_test), y_test)\n",
        "\n",
        "  table[0][k] = test_mae\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y17QdNPqCKjD"
      },
      "source": [
        "Discussion\n",
        "\n",
        "Interestingly, the lowerst MAE was achieved with a k of 2, and it was 2.68. Comparing it to problem 3.2, it performed slightly words than 3.2 where k was 3 rather than 3, and distance weighting and normalization were also both used. This is a good case where the luck of the train/test split made a slight difference. Moreso than in classification, the MAE gradually dropped as k was increased past 2.\n",
        "\n",
        "Also with all k values tried here (from 1 to 15), each still performed better than when not doing normalization or distance weighting (look at 3.2 above) where the training MAE was 4.57.\n",
        "\n",
        "Again, this test was a good reminder that it is important to experiment with different values for hyperparameters, including k, with each problem you are solving with ML. There isn't always a perfect default, so it helps to find what works best for your problem. This is explained by the no-free-lunch theorem which states that and inductive bias that is good for some problems will probably be bad for equally as many problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vFBs0SmCKjD"
      },
      "source": [
        "## 4. (20%) KNN with nominal and real data\n",
        "\n",
        "- Use the [lymph dataset](https://axon.cs.byu.edu/data/uci_class/lymph.arff)\n",
        "- Use a 80/20 split of the data for the training/test set\n",
        "- This dataset has both continuous and nominal attributes\n",
        "- Implement a distance metric which uses Euclidean distance for continuous features and 0/1 distance for nominal. Hints:\n",
        "    - Write your own distance function (e.g. mydist) and use clf = KNeighborsClassifier(metric=mydist)\n",
        "    - Change the nominal features in the data set to integer values since KNeighborsClassifier expects numeric features. I used Label_Encoder on the nominal features.\n",
        "    - Keep a list of which features are nominal which mydist can use to decide which distance measure to use\n",
        "    - There was an occasional bug in SK version 1.3.0 (\"Flags object has no attribute 'c_contiguous'\") that went away when I upgraded to the lastest SK version 1.3.1\n",
        "- Use your own choice for k and other parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_dist(a: ndarray, b: ndarray, nominal_feature_indexes: set) -> float:\n",
        "  \"\"\"\n",
        "  dist = euclidean_dist(continuous_features) + nominal_dist(nominal_features)\n",
        "  \"\"\"\n",
        "  # separate into lists of nominal and continuous features\n",
        "  a_nominal = []\n",
        "  b_nominal = []\n",
        "  a_cont = []\n",
        "  b_cont = []\n",
        "  for i in range(len(a)):\n",
        "    if i in nominal_feature_indexes:\n",
        "      a_nominal.append(a[i])\n",
        "      b_nominal.append(b[i])\n",
        "    else:\n",
        "      a_cont.append(a[i])\n",
        "      b_cont.append(b[i])\n",
        "\n",
        "  # add distance for nominal and continuous features\n",
        "  dist = 0.0\n",
        "  dist += nominal_dist(a_nominal, b_nominal)\n",
        "  dist += continuous_dist(a_cont, b_cont)\n",
        "\n",
        "  return dist\n",
        "\n",
        "def nominal_dist(a: List[float], b: list[float]) -> int:\n",
        "  '''0/1 Distance (0 if they match, 1 otherwise)'''\n",
        "  assert(len(a) == len(b))\n",
        "\n",
        "  dist = 0\n",
        "  for i in range(len(a)):\n",
        "    if a[i] != b[i]:\n",
        "      dist += 1\n",
        "\n",
        "  return dist\n",
        "\n",
        "def continuous_dist(a: List[float], b: list[float]) -> float:\n",
        "  '''Euclidean Distance = sqrt(sum (1 to m) (x_i - y_i)^2)'''\n",
        "  assert(len(a) == len(b))\n",
        "\n",
        "  sum = 0.0\n",
        "  for i in range(len(a)):\n",
        "    sum += (a[i] - b[i]) ** 2\n",
        "\n",
        "  return math.sqrt(sum)"
      ],
      "metadata": {
        "id": "zfemeI4H3yr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrydTVRsCKjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2352a933-6f63-4a03-be4d-4482493d09c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial    Train Score         Test Score          Train Score         Test Score\n",
            "-------  ------------------  ------------------  ------------------  ------------------\n",
            "         My Dist                                 Default Dist\n",
            "0        0.923728813559322   0.9                 0.8898305084745762  0.7\n",
            "1        0.9067796610169492  0.8                 0.8813559322033898  0.7333333333333333\n",
            "2        0.9067796610169492  0.7666666666666667  0.8898305084745762  0.7\n",
            "3        0.940677966101695   0.8666666666666667  0.8559322033898306  0.8\n",
            "4        0.9067796610169492  0.9333333333333333  0.8898305084745762  0.7666666666666667\n",
            "5        0.923728813559322   0.8333333333333334  0.9067796610169492  0.7\n",
            "6        0.9152542372881356  0.8                 0.923728813559322   0.7\n",
            "7        0.9152542372881356  0.8666666666666667  0.8898305084745762  0.7\n",
            "8        0.9576271186440678  0.7666666666666667  0.9322033898305084  0.7\n",
            "9        0.9067796610169492  0.8                 0.8898305084745762  0.7\n",
            "Avg      0.9203389830508476  0.8333333333333333  0.894915254237288   0.72\n"
          ]
        }
      ],
      "source": [
        "# Train/Predict lymph with your own distance metric\n",
        "X, y = load_lymph()\n",
        "lymph_nominal_feature_indexes = set([0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16])\n",
        "\n",
        "# normalize continuous features\n",
        "X = normalize_continuous_features(X, lymph_nominal_feature_indexes)\n",
        "\n",
        "# label encode nominal features in X\n",
        "for i in range(X.shape[1]):\n",
        "  if i in lymph_nominal_feature_indexes:\n",
        "    X[:, i] = LabelEncoder().fit_transform(X[:, i])\n",
        "# label encode y\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "headers = [\"Trial\", \"Train Score\", \"Test Score\", \"Train Score\", \"Test Score\"]\n",
        "table = [[\"\", \"My Dist\", \"\", \"Default Dist\", \"\"]]\n",
        "\n",
        "avg_train_acc, avg_test_acc = 0.0, 0.0\n",
        "n_trials = 10\n",
        "for i in range(n_trials):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "  knn_my_dist = KNeighborsClassifier( # using my_dist\n",
        "    n_neighbors=3,\n",
        "    metric=my_dist,\n",
        "    metric_params={'nominal_feature_indexes': lymph_nominal_feature_indexes}\n",
        "  ).fit(X_train, y_train)\n",
        "\n",
        "  knn_default_dist = KNeighborsClassifier( # using defualt dist\n",
        "    n_neighbors=3,\n",
        "  ).fit(X_train, y_train)\n",
        "\n",
        "  train_acc_my_dist = knn_my_dist.score(X_train, y_train)\n",
        "  test_acc_my_dist = knn_my_dist.score(X_test, y_test)\n",
        "  train_acc_default_dist = knn_default_dist.score(X_train, y_train)\n",
        "  test_acc_default_dist = knn_default_dist.score(X_test, y_test)\n",
        "\n",
        "  table.append([i, train_acc_my_dist, test_acc_my_dist, train_acc_default_dist, test_acc_default_dist])\n",
        "\n",
        "avg_train_my_dist = sum([table[row][1] for row in range(1, n_trials + 1)]) / n_trials\n",
        "avg_test_my_dist = sum([table[row][2] for row in range(1, n_trials + 1)]) / n_trials\n",
        "avg_train_defualt_dist = sum([table[row][3] for row in range(1, n_trials + 1)]) / n_trials\n",
        "avg_test_defualt_dist = sum([table[row][4] for row in range(1, n_trials + 1)]) / n_trials\n",
        "\n",
        "table.append([\"Avg\", avg_train_my_dist, avg_test_my_dist, avg_train_defualt_dist, avg_test_defualt_dist])\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krpD3flRCKjD"
      },
      "source": [
        "*Explain your distance metric and discuss your results*\n",
        "\n",
        "I am pretty happy with my results. To my metric function uses 0/1 distance for nominal features and euclidean distance for continuous features. The way I implemented this was by creating a distance function `my_dist`, which calls two helper functions, one for 0/1 and the other for euclidian distance. When trying to figure out the best way to do this, I was initially thinking of calling the euclidean distance function separately for each continuous feature, but I then realized that this wouldn't work right with the euclidean function, so at the beginning of `my_dist` I group all continuous features together, and all nonimal features together and send them into their respective functions all at the same time. For the nominal features, it doesn't make any difference doing them together or separate, because you would end up with the same resulting total either way.\n",
        "\n",
        "I create a set of indexes of nominal features for the dataset which I pass into the distance function using the KNeighborsClassifier `metric_params` agrument which accepts an object containing parameters used in the metric function (in this case, `my_dist` which accepts a set `nominal_feature_indexes` as an argument).\n",
        "\n",
        "I also realized it would be important to normalize all continuous features so that large distances wouldn't minimalize the distances from continuous features. I created a function `normalize_continuous_features` to do this, which also accepts `nominal_feature_indexes` to know which features to normalize (all those not appearing in `nominal_feature_indexes`).\n",
        "\n",
        "My distance function actually outperformed the default one! My training set accuracy was about 3% higher than the default and the test set accuracy was about 11% better. One reason that my function did better is that the default distance function in Minkowski, and as shown in 1.1 above, using a different p-value could have improved its results. Additionally, my function knows how to treat nominal and continuous data differently, meaning that nominal data doesn't incorrectly make assumptions about distance that can't rightfully be made. For example, if 2 nominal features are 1 and 16, a continuous distance algorithm might treat them as relatively far apart, when in reality, the distance is no different than when comparing nominal values 1 and 2. Due to these reasons, my distance function performed pretty well!\n",
        "\n",
        "If I had time to further tune my distance function, my next step would be to figure out a constant weight for both the nominal and continuous total distances. For example, the nominal distance may output a value of 1 while the continuous distance may be 2 when the more accurate value is one for both of them. Finding out which direction there is a skew will allow me to add a weight to one of them to make sure that both are balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrSjm-sACKjD"
      },
      "source": [
        "## 5. (Optional 15% extra credit) Code up your own KNN Learner\n",
        "Below is a scaffold you could use if you want. Requirements for this task:\n",
        "- Your model should support the methods shown in the example scaffold below\n",
        "- Use Euclidean distance to decide closest neighbors\n",
        "- Implement both the classification and regression versions\n",
        "- Include optional distance weighting for both algorithms\n",
        "- Run your algorithm on the magic telescope and housing data sets above and discuss and compare your results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxFotnBsCKjD"
      },
      "source": [
        "*Discussion*\n",
        "\n",
        "For my KNNClassifier and KNNRegressor, I decided to create a KNNBase class to avoid any code duplication. There, I create the main methods `__init__`, `fit`, `predict`, and `score`. `__init__` and `fit` are exactly the same for both classes. `predict` and `score` however, do have some differences. Do eliminate as much duplication as possible, I used the template method pattern for those two methods, and simply had `_predict` and `_score` as abstract methods for the child classes to implement.\n",
        "\n",
        "I ended up using the same distance function as I created in step 4. This uses Euclidean distance for real features and 0/1 distance for categorical features.\n",
        "\n",
        "I was happy that implementing `fit` was really simple. One of the benefits of KNN is that it doesn't actually require training. You simple have to save the datapoints. For the `score` function, in classification, I simply return the average accuracy, and for regression, I used the regression function from the slides: `f(x_q) = (sum (i = 1 to k) (w_i * f(X_i))) / (sum (i = 1 to k) (w_i))`, which works quite well.\n",
        "\n",
        "For classification, the MagicTelescope dataset is so large and my algorithm isn't particularly efficient and the dataset is large, so I decided to not check training accuracy because the training set is so large that computation time would take a while. Looking at the results, my classifier scored almost exactly the same as Scikit Learn's. With both no distance weighting and distance weighting, both models scored about 83% and only varied by about 0.4%. I think this is likely to do with the fact that the dataset is quite large. The results demonstrate that my classifier works fairly well!\n",
        "\n",
        "For the regressor, my results were close to Scikit-Learn's. For no distance weighting, I scored 1.98 for train MAE, while Scikit's scored 2.03, so I actually did better on that. For test MAE, mine was 3.26, and Scikit's was 2.79, meaning that Scikit's was quite a bit better than mine. Considering the fact that the housing values ranged from about 8 to about 50, 3.26 MAE really isn't that bad, and shows that my algorithm was working properly. When using inverse distance weighting, both models scored 0 MAE for training data. This is expected, because , once again, since training data was being used for testing, each point tested had a perfectly cooresponding neighbor in the data. When a neighbor is at the same spot (same feature values), the weight becomes infinity, so that feature's value is returned, meaning that there will NEVER be any error. For test values with distance weighting, mine had a MAE of 2.79, and Scikit's was 2.63, so mine was almost the same. I did notice that the train-test split did affect the results a good amount, so if I had more time, I would run several trials on my regressor and get averages.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataType(Enum):\n",
        "  real = 1\n",
        "  categorical = 2\n",
        "\n",
        "class KNNBase(ABC, BaseEstimator,ClassifierMixin):\n",
        "  def __init__(self, columntype: list[DataType], weight_type='inverse_distance', k=3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      columntype for each column tells you if continues[real] or if nominal[categorical].\n",
        "      weight_type: inverse_distance voting or if non distance weighting. Options = [\"no_weight\",\"inverse_distance\"]\n",
        "      k: the number of neighbors to look at\n",
        "    \"\"\"\n",
        "    self.columntype = columntype\n",
        "    self.weight_type = weight_type\n",
        "    self.k = k\n",
        "\n",
        "  def fit(self, data, labels):\n",
        "    \"\"\" Fit the data; run the algorithm (for this lab really just saves the data :D)\n",
        "    Args:\n",
        "      X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "      y (array-like): A 2D numpy array with the training targets\n",
        "    Returns:\n",
        "      self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "    \"\"\"\n",
        "    assert(data.shape[0] == len(labels))\n",
        "\n",
        "    self.X = data\n",
        "    self.y = labels\n",
        "    self.n_training_samples = len(labels)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def predict(self, data):\n",
        "    \"\"\" Predict all classes/regression values for a dataset X\n",
        "    Args:\n",
        "      X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "    Returns:\n",
        "      array, shape (n_samples,)\n",
        "        Predicted target values per element in X.\n",
        "    \"\"\"\n",
        "    n = data.shape[0]\n",
        "\n",
        "    res = [0] * n\n",
        "    for i in range(n):\n",
        "      res[i] = self._predict(data[i])\n",
        "\n",
        "    return res\n",
        "\n",
        "  #Returns the Mean score given input data and labels\n",
        "  def score(self, X, y):\n",
        "    \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
        "    Args:\n",
        "      X (array-like): A 2D numpy array with data, excluding targets\n",
        "      y (array-like): A 2D numpy array with targets\n",
        "    Returns:\n",
        "      score : float\n",
        "        Mean accuracy of self.predict(X) for Classification\n",
        "        Mean absolute error (MAE) for regression\n",
        "    \"\"\"\n",
        "    predictions = self.predict(X)\n",
        "\n",
        "    return self._score(predictions, y)\n",
        "\n",
        "  @abstractmethod\n",
        "  def _predict(self, sample: ndarray):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def _score(self, predictions: list, y: ndarray):\n",
        "    pass\n",
        "\n",
        "  def _get_distances_to_all_samples(self, sample: ndarray) -> ndarray:\n",
        "    '''Returns distance to each training sample, cooresponding by index'''\n",
        "    distances = np.full(self.n_training_samples, np.inf)\n",
        "    for i in range(self.n_training_samples):\n",
        "      distances[i] = self._compute_distance(sample, self.X[i])\n",
        "\n",
        "    return distances\n",
        "\n",
        "  def _get_indexes_knn(self, sample: ndarray, distances: ndarray) -> ndarray:\n",
        "    '''Returns: the indexes of the k-nearest neighbors'''\n",
        "    assert(len(distances) == len(self.y))\n",
        "\n",
        "    return np.argsort(distances)[:self.k]\n",
        "\n",
        "  def _compute_distance_weight(self, dist: float) -> float:\n",
        "    if dist == 0: return float('inf')\n",
        "\n",
        "    return 1 / (dist ** 2)\n",
        "\n",
        "  def _compute_distance(self, a: ndarray, b: ndarray) -> float:\n",
        "    \"\"\"\n",
        "    dist = euclidean_dist(real_features) + nominal_dist(categorcal_features)\n",
        "\n",
        "    Args:\n",
        "      a: the features from a sample\n",
        "      b: the features from another sample\n",
        "    Returns:\n",
        "      the distance from a to b\n",
        "    \"\"\"\n",
        "    # separate into lists of nominal and continuous features\n",
        "    a_nominal, b_nominal = [], []\n",
        "    a_cont, b_cont = [], []\n",
        "    for i in range(len(a)):\n",
        "      if self.columntype[i] == DataType.categorical:\n",
        "        a_nominal.append(a[i])\n",
        "        b_nominal.append(b[i])\n",
        "      else:\n",
        "        a_cont.append(a[i])\n",
        "        b_cont.append(b[i])\n",
        "\n",
        "    # add distance for nominal and continuous features\n",
        "    dist = self._compute_nominal_dist(a_nominal, b_nominal) + self._compute_continuous_dist(a_cont, b_cont)\n",
        "\n",
        "    return dist\n",
        "\n",
        "  def _compute_nominal_dist(self, a: List[float], b: list[float]) -> int:\n",
        "    '''0/1 Distance (0 if they match, 1 otherwise)'''\n",
        "    assert(len(a) == len(b))\n",
        "\n",
        "    dist = 0\n",
        "    for i in range(len(a)):\n",
        "      if a[i] != b[i]:\n",
        "        dist += 1\n",
        "\n",
        "    return dist\n",
        "\n",
        "  def _compute_continuous_dist(self, a: List[float], b: list[float]) -> float:\n",
        "    '''Euclidean Distance = sqrt(sum (1 to m) (x_i - y_i)^2)'''\n",
        "    assert(len(a) == len(b))\n",
        "\n",
        "    sum = 0.0\n",
        "    for i in range(len(a)):\n",
        "      sum += (a[i] - b[i]) ** 2\n",
        "\n",
        "    return math.sqrt(sum)"
      ],
      "metadata": {
        "id": "_zdY8b2kk-pT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "7ftQ4WDiCKjD"
      },
      "outputs": [],
      "source": [
        "class KNNClassifier(KNNBase):\n",
        "  def __init__(self, columntype: list[DataType], weight_type='inverse_distance', k=3):\n",
        "    super().__init__(columntype, weight_type, k)\n",
        "\n",
        "  def _predict(self, sample: ndarray):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      sample: an array containing all features from which to predict the class\n",
        "    \"\"\"\n",
        "    distances = self._get_distances_to_all_samples(sample)\n",
        "    indexes_knn = self._get_indexes_knn(sample, distances)\n",
        "\n",
        "    if self.weight_type == 'inverse_distance': # inverse distance weighting\n",
        "      clss = self._predict_inverse_distance_weighting(distances, indexes_knn)\n",
        "    else: # no weighting\n",
        "      clss = self._predict_no_weighting(distances, indexes_knn)\n",
        "\n",
        "    return clss\n",
        "\n",
        "  def _score(self, predictions: list, y: ndarray):\n",
        "    matching = 0\n",
        "    for i in range(len(y)):\n",
        "      if y[i] == predictions[i]:\n",
        "        matching += 1\n",
        "\n",
        "    return matching / len(y)\n",
        "\n",
        "  def _predict_inverse_distance_weighting(self, distances: ndarray, indexes_knn: ndarray):\n",
        "    '''Returns the class with the highest total weight among knns'''\n",
        "    # sum weight of each class from respective knns\n",
        "    class_weights = defaultdict(float)\n",
        "    for i in indexes_knn:\n",
        "      clss = self.y[i]\n",
        "      dist = distances[i]\n",
        "      weight = self._compute_distance_weight(dist)\n",
        "\n",
        "      class_weights[clss] += weight\n",
        "\n",
        "    return max(class_weights, key=class_weights.get)\n",
        "\n",
        "  def _predict_no_weighting(self, distances: ndarray, indexes_knn: ndarray):\n",
        "    class_counts = defaultdict(int)\n",
        "    for i in indexes_knn:\n",
        "      clss = self.y[i]\n",
        "\n",
        "      class_counts[clss] += 1\n",
        "\n",
        "    return max(class_counts, key=class_counts.get)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNRegressor(KNNBase):\n",
        "  def __init__(self, columntype: list[DataType], weight_type='inverse_distance', k=3):\n",
        "    super().__init__(columntype, weight_type, k)\n",
        "\n",
        "  def _predict(self, sample: ndarray):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      sample: an array containing all features from which to predict the regression value\n",
        "    \"\"\"\n",
        "    distances = self._get_distances_to_all_samples(sample)\n",
        "    indexes_knn = self._get_indexes_knn(sample, distances)\n",
        "\n",
        "    # get weights for each\n",
        "    knn_weights = np.full(len(indexes_knn), 1.0)\n",
        "    if self.weight_type == 'inverse_distance':\n",
        "      # inverse distance weighting\n",
        "      for i, knn_index in enumerate(indexes_knn):\n",
        "        knn_weights[i] = self._compute_distance_weight(distances[knn_index])\n",
        "\n",
        "    return self._compute_regression_value(indexes_knn, knn_weights)\n",
        "\n",
        "  def _score(self, predictions: list, y: ndarray):\n",
        "    '''Returns MAE (Mean absolute error)'''\n",
        "    error = 0.0\n",
        "    for i in range(len(y)):\n",
        "      error += abs(y[i] - predictions[i])\n",
        "\n",
        "    return error / len(y)\n",
        "\n",
        "  def _compute_regression_value(self, indexes_knn: ndarray, knn_weights: ndarray):\n",
        "    # f(x_q) = (sum (i = 1 to k) (w_i * f(X_i))) / (sum (i = 1 to k) (w_i))\n",
        "    if float('inf') in knn_weights:\n",
        "      # infinity weight (just return the y value of the sample with inf weight)\n",
        "      inf_knn_i = np.where(knn_weights == float('inf'))\n",
        "      inf_i = indexes_knn[inf_knn_i]\n",
        "\n",
        "      return self.y[inf_i].item()\n",
        "\n",
        "    top = sum(knn_weights[i] * self.y[knn_index] for i, knn_index in enumerate(indexes_knn))\n",
        "    bottom = sum(knn_weights)\n",
        "\n",
        "    return top / bottom"
      ],
      "metadata": {
        "id": "lzcAt4llkhS4"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CLASSIFICATION: SIMPLE EXAMPLE\n",
        "X = np.array([[.3, .8], [-.3, 1.6], [.9, 0], [1, 1]])\n",
        "y = np.array(['A', 'B', 'B', 'A'])\n",
        "\n",
        "columntype = [DataType.real, DataType.real]\n",
        "\n",
        "knn = KNNClassifier(columntype=columntype, weight_type='no_weight', k=3)\n",
        "knn.fit(X, y)\n",
        "\n",
        "test_samples = np.array([[0.5, 0.2]])\n",
        "print(knn.predict(test_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTQXEm_THPyC",
        "outputId": "6639479f-5619-41fb-99fe-b231c07fc0bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CLASSIFICATION: MAGIC TELESCOPE\n",
        "trials = [\n",
        "    {'weight_type': 'no_weight', 'title': 'Magic Telescope: No distance weighting'},\n",
        "    {'weight_type': 'inverse_distance', 'title': 'Magic Telescope: Inverse distance weighting'}\n",
        "]\n",
        "\n",
        "X, y = load_magic_telescope()\n",
        "X = normalize(X)\n",
        "columntype = [DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real]\n",
        "\n",
        "for trial in trials:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "  knn = KNNClassifier(columntype=columntype, weight_type=trial['weight_type'], k=3)\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "  print(trial['title'])\n",
        "  print(\"   test score: \", str(test_acc))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIaGM5wYacsD",
        "outputId": "c46d7e2a-11b6-4919-de4a-3e7017d07ccd"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magic Telescope: No distance weighting\n",
            "   test score:  0.8322818086225027\n",
            "\n",
            "Magic Telescope: Inverse distance weighting\n",
            "   test score:  0.8354363827549948\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST REGRESSION: SIMPLE EXAMPLE\n",
        "X = np.array([[.3, .8], [-.3, 1.6], [.9, 0], [1, 1]])\n",
        "y = np.array([.6, -.3, .8, 1.2])\n",
        "\n",
        "columntype = [DataType.real, DataType.real]\n",
        "\n",
        "knn = KNNRegressor(columntype=columntype, weight_type='no_weight', k=3)\n",
        "knn.fit(X, y)\n",
        "\n",
        "test_samples = np.array([[0.5, 0.2]])\n",
        "print(\"prediction: \", str(knn.predict(test_samples)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfUTTRvLwDbE",
        "outputId": "40836898-d90e-4088-d732-0b89e4f0219b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction:  [0.8666666666666666]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST REGRESSION: HOUSING\n",
        "trials = [\n",
        "    {'weight_type': 'no_weight', 'title': 'Housing: No distance weighting'},\n",
        "    {'weight_type': 'inverse_distance', 'title': 'Housing: Inverse distance weighting'}\n",
        "]\n",
        "\n",
        "X, y = load_housing()\n",
        "X = normalize(X)\n",
        "columntype = [DataType.real, DataType.real, DataType.real, DataType.categorical, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real, DataType.real]\n",
        "\n",
        "for trial in trials:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  knn = KNNRegressor(columntype=columntype, weight_type=trial['weight_type'], k=3)\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  train_acc = knn.score(X_train, y_train)\n",
        "  test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "  print(trial['title'])\n",
        "  print(\"   train MAE: \", str(train_acc))\n",
        "  print(\"   test MAE: \", str(test_acc))\n",
        "  print()"
      ],
      "metadata": {
        "id": "TmDozKl_jGgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c972f43a-7522-4fc4-bb10-72b333b58390"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Housing: No distance weighting\n",
            "   train MAE:  1.9885313531353124\n",
            "   test MAE:  3.267647058823527\n",
            "\n",
            "Housing: Inverse distance weighting\n",
            "   train MAE:  0.0\n",
            "   test MAE:  2.6542582520705964\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}